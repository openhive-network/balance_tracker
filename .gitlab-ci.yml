stages:
- lint
- build
- sync
- test
- cleanup
- publish

variables:
  # Git configuration
  # Fetch strategy reuses workspace between jobs, reducing GitLab server load.
  # Full clone (depth 0) enables efficient incremental fetches - shallow clones
  # don't reduce server CPU and make fetch less effective.
  GIT_STRATEGY: fetch
  GIT_SUBMODULE_STRATEGY: recursive
  GIT_DEPTH: 0
  GIT_SUBMODULE_DEPTH: 0
  GIT_SUBMODULE_UPDATE_FLAGS: --jobs 4
  # Temporary: separate clone path prevents clone-strategy jobs from erasing
  # fetch workspaces during transition. Remove once all projects use fetch.
  GIT_CLONE_PATH: $CI_BUILDS_DIR/fetch/$CI_RUNNER_SHORT_TOKEN/$CI_CONCURRENT_ID/$CI_PROJECT_PATH
  # HAF configuration
  DATA_CACHE_HAF_PREFIX: "/cache/replay_data_haf"
  # NFS cache configuration for sync data sharing across builders
  DATA_CACHE_NFS_PREFIX: "/nfs/ci-cache"
  # Cache keys include both HAF and balance_tracker commits for proper invalidation
  BTRACKER_CACHE_KEY: "${HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}"
  # Cache types for different replay states
  BTRACKER_SYNC_CACHE_TYPE: "btracker_sync"   # HAF + balance_tracker synced
  BTRACKER_MOCK_CACHE_TYPE: "btracker_mock"   # HAF + balance_tracker + mock data
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m
  FF_NETWORK_PER_BUILD: 1
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu24.04-10
  BUILDER_IMAGE_TAG: "$TEST_HAF_IMAGE_TAG"
  BUILDER_IMAGE_PATH: "registry.gitlab.syncad.com/hive/haf/ci-base-image${BUILDER_IMAGE_TAG}"
  # HAF submodule commit - must match the 'ref:' in the include section below
  # This is needed for service containers which can't access dotenv artifacts
  HAF_COMMIT: "9add4c306ddc0067bf00857c2c70053f21525ae6"
  # Enable CI-specific PostgreSQL config with reduced memory for HAF service containers
  HAF_CI_MODE: "1"

include:
- template: Workflows/Branch-Pipelines.gitlab-ci.yml
- project: hive/haf
  ref: 9add4c306ddc0067bf00857c2c70053f21525ae6   # develop
  file: /scripts/ci-helpers/prepare_data_image_job.yml
  # Do not include common-ci-configuration here, it is already referenced by scripts/ci-helpers/prepare_data_image_job.yml included from Haf/Hive repos

default:
  hooks:
    pre_get_sources_script:
      # Clean corrupt git state left by cancelled pipelines (see GitLab #296638, #4600)
      # Wrapped in subshell to avoid changing working directory for subsequent git operations
      - |
        (
        cd "${CI_PROJECT_DIR:-/builds}" 2>/dev/null || exit 0
        # Allow file:// transport for nested submodule operations (git 2.38.1+ security)
        git config --global protocol.file.allow always 2>/dev/null || true
        echo "pre_get_sources: checking $(pwd) for corrupt git state"
        if [ -d ".git" ]; then
          # Remove stale lock files that block git operations
          find .git -name "*.lock" -delete 2>/dev/null || true

          # Check if main repo is corrupt - if so, remove .git to force fresh clone
          if ! git rev-parse HEAD >/dev/null 2>&1; then
            echo "pre_get_sources: main repository corrupt, forcing fresh clone"
            rm -rf .git
          else
            # Main repo OK - check and clean corrupt submodules
            # Check both the working dir and .git/modules/ since either can be corrupt
            if [ -f ".gitmodules" ]; then
              git config --file .gitmodules --get-regexp path 2>/dev/null | awk '{print $2}' | while read submod; do
                needs_clean=false
                [ -z "$submod" ] && continue
                # Check if submodule working directory exists but is corrupt
                if [ -d "$submod" ] && [ -f "$submod/.git" ]; then
                  if ! git -C "$submod" rev-parse HEAD >/dev/null 2>&1; then
                    needs_clean=true
                  fi
                fi
                # Check if .git/modules exists but is corrupt (even if working dir is gone)
                if [ -d ".git/modules/$submod" ]; then
                  if ! git --git-dir=".git/modules/$submod" rev-parse HEAD >/dev/null 2>&1; then
                    echo "pre_get_sources: $submod corrupt (rev-parse failed)"
                    needs_clean=true
                  fi
                fi
                if [ "$needs_clean" = true ]; then
                  echo "pre_get_sources: cleaning corrupt submodule: $submod"
                  rm -rf "$submod" ".git/modules/$submod"
                fi
              done
            else
              echo "pre_get_sources: no .gitmodules file"
            fi

            # Handle directory-to-submodule transitions inside submodules
            # Known paths that were converted from directories to submodules in hive
            # This list matches hive's .gitmodules - clean if they exist as directories
            if [ -d "haf" ]; then
              echo "pre_get_sources: checking haf for directory-to-submodule transitions"
              for nested in hive/libraries/fc hive/libraries/appbase; do
                nested_path="haf/$nested"
                modules_path=".git/modules/haf/modules/$nested"
                # If path exists as directory but not as a submodule, remove it
                if [ -d "$nested_path" ] && [ ! -f "$nested_path/.git" ]; then
                  echo "pre_get_sources: removing stale directory for submodule: $nested_path"
                  rm -rf "$nested_path"
                  # Also clean the .git/modules entry if it exists
                  if [ -d "$modules_path" ]; then
                    echo "pre_get_sources: removing stale .git/modules entry: $modules_path"
                    rm -rf "$modules_path"
                  fi
                fi
              done

              # Check if haf/hive submodule has corrupt origin (missing remote config)
              hive_modules=".git/modules/haf/modules/hive"
              if [ -d "$hive_modules" ]; then
                if ! git --git-dir="$hive_modules" remote get-url origin >/dev/null 2>&1; then
                  echo "pre_get_sources: haf/hive has corrupt origin, cleaning"
                  rm -rf "haf/hive" "$hive_modules"
                fi
              fi
            fi

            echo "pre_get_sources: existing repo OK"
          fi
        else
          echo "pre_get_sources: no .git directory (fresh workspace)"
        fi
        )

.lint_job:
  stage: lint
  variables:
    GIT_SUBMODULE_STRATEGY: none
  artifacts:
    name: lint-results
    when: always
  tags:
  - public-runner-docker

lint_bash_scripts:
  extends: .lint_job
  image: koalaman/shellcheck-alpine:latest
  before_script:
  - apk add xmlstarlet
  script:
  # Exclude .git and haf submodule (submodules may be stale from other jobs using fetch strategy)
  - find . -name .git -type d -prune -o -path ./haf -prune -o -type f -name \*.sh -exec shellcheck -f checkstyle
    {} + | tee shellcheck-checkstyle-result.xml
  after_script:
  - xmlstarlet tr misc/checkstyle2junit.xslt shellcheck-checkstyle-result.xml > shellcheck-junit-result.xml
  artifacts:
    paths:
    - shellcheck-checkstyle-result.xml
    - shellcheck-junit-result.xml
    reports:
      junit: shellcheck-junit-result.xml

lint_sql_scripts:
  extends: .lint_job
  image:
    name: sqlfluff/sqlfluff:2.1.4
    entrypoint: [""]
  script:
  - sqlfluff lint --format yaml --write-output sql-lint.yaml
  artifacts:
    paths:
    - sql-lint.yaml

validate_haf_commit:
  stage: build
  image: alpine:latest
  script:
  - |
    set -e
    apk add --no-cache git
    # Validate that HAF_COMMIT variable matches both the submodule and include ref
    # This prevents cache misses due to mismatched commits
    SUBMODULE_COMMIT=$(cat .git/modules/haf/HEAD 2>/dev/null || git -C haf rev-parse HEAD)
    INCLUDE_REF=$(grep -A2 "project:.*hive/haf" .gitlab-ci.yml | grep "ref:" | head -1 | sed 's/.*ref: *\([a-f0-9]*\).*/\1/' || true)

    echo "HAF_COMMIT variable: $HAF_COMMIT"
    echo "HAF submodule HEAD:  $SUBMODULE_COMMIT"
    echo "Include ref:         $INCLUDE_REF"

    ERRORS=0
    if [ "$HAF_COMMIT" != "$SUBMODULE_COMMIT" ]; then
      echo "ERROR: HAF_COMMIT variable does not match submodule commit!"
      echo "       Update HAF_COMMIT in .gitlab-ci.yml to: $SUBMODULE_COMMIT"
      ERRORS=1
    fi
    if [ "$HAF_COMMIT" != "$INCLUDE_REF" ]; then
      echo "ERROR: HAF_COMMIT variable does not match include ref!"
      echo "       Both should be: $HAF_COMMIT"
      ERRORS=1
    fi
    if [ $ERRORS -eq 1 ]; then
      echo ""
      echo "To fix: ensure HAF_COMMIT, include ref, and submodule all use the same commit"
      exit 1
    fi
    echo "All HAF commit references are consistent"
  tags:
  - public-runner-docker

prepare_haf_image:
  stage: build
  extends: .prepare_haf_image
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$HAF_DEPLOY_USERNAME"
    REGISTRY_PASS: "$HAF_DEPLOY_TOKEN"
  before_script:
  - git config --global --add safe.directory $CI_PROJECT_DIR/haf
  tags:
  - public-runner-docker
  - build-mainnet

extract-swagger-json:
  extends: .filter_out_swagger_json
  stage: build
  variables:
    INPUT_SQL_SWAGGER_FILE: "${CI_PROJECT_DIR}/endpoints/endpoint_schema.sql"
  tags:
  - public-runner-docker

generate_python_api_client:
  extends: .project_develop_configuration_template
  stage: build
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  needs:
  - job: extract-swagger-json
    artifacts: true
  script:
  - ${PYPROJECT_DIR}/generate_balance_tracker_api_client.sh
  artifacts:
    paths:
    - "${PYPROJECT_DIR}/balance_api"
    - "${PYPROJECT_DIR}/balance_api/balance_api_client"
  tags:
  - public-runner-docker

build_python_api_client_wheel:
  extends: .build_wheel_template
  stage: build
  needs:
  - job: generate_python_api_client
    artifacts: true
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  tags:
  - public-runner-docker

generate-wax-spec:
  extends: .generate_swagger_package
  stage: build
  variables:
    # Use artifact path directly - dotenv BUILT_JSON_SWAGGER_FILE contains absolute
    # path from previous job's runner which doesn't exist on this runner
    INPUT_JSON_SWAGGER_FILE: "${CI_PROJECT_DIR}/build/swagger-doc.json"
    API_TYPE: "rest"
    NAMESPACE: "balance_tracker"
    NPM_PACKAGE_SCOPE: "@hiveio"
    NPM_PACKAGE_NAME: "wax-api-balance-tracker"
  needs:
  - job: extract-swagger-json
    artifacts: true
  tags:
  - public-runner-docker

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
  - job: prepare_haf_image
    artifacts: true
  stage: build
  timeout: 80m
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/docker/config_5M.ini"
  tags:
  - data-cache-storage
  - fast

.docker-build-template:
  extends: .docker_image_builder_job_template
  stage: build
  variables:
    BASE_REPO_NAME: ""
    BASE_TAG: ""
    NAME: ""
    TARGET: "$NAME"
    PROGRESS_DISPLAY: "plain"
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  script:
  - |
    echo -e "\e[0Ksection_end:$(date +%s):tag\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):build[collapsed=true]\r\e[0KBaking $NAME${BASE_REPO_NAME:+/$BASE_REPO_NAME} image..."
    function image-exists() {
      local image=$1
      docker manifest inspect "$1" > /dev/null
      return $?
    }
    if image-exists "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:$BASE_TAG"; then
      echo "Image $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG} already exists. Skipping..."
      if [[ -n "$CI_COMMIT_TAG" && "$TARGET" == "full-ci" ]]; then
        echo "Tagging pre-existing image with Git tag..."
        docker pull "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG}"
        docker tag "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG}" "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
        docker push "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
      fi
    else
      echo "Baking $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG} image..."
      git config --global --add safe.directory $(pwd)
      scripts/ci-helpers/build_docker_image.sh "$CI_PROJECT_DIR"
    fi
    echo -e "\e[0Ksection_end:$(date +%s):build\r\e[0K"
  tags:
  - public-runner-docker
  - build-mainnet

docker-ci-runner-build:
  extends: .docker-build-template
  variables:
    BASE_REPO_NAME: ""
    BASE_TAG: "docker-24.0.1-16"
    NAME: "ci-runner"
    TARGET: "ci-runner-ci"

docker-setup-docker-image-build:
  extends: .docker-build-template
  variables:
    GIT_SUBMODULE_STRATEGY: none
    GIT_DEPTH: 1
    BASE_REPO_NAME: ""
    BASE_TAG: "$CI_COMMIT_SHORT_SHA"
    NAME: ""
    TARGET: "full-ci"

sync:
  extends: .docker_image_builder_job_template
  stage: sync
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - prepare_haf_image
  - prepare_haf_data
  - docker-setup-docker-image-build
  - docker-ci-runner-build
  variables:
    DATA_SOURCE: ${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMPOSE_OPTIONS_STRING: --env-file ci.env --file docker-compose.yml --file overrides/ci.yml
      --ansi never
  timeout: 1 hours
  before_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
    git config --global --add safe.directory "$CI_PROJECT_DIR"
    git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
    echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  - |
    # Check for existing btracker sync cache first (HAF + balance_tracker already synced)
    # If found, we can skip the sync entirely since the cache key includes both commits
    LOCAL_BTRACKER_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    LOCAL_HAF_CACHE="${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    CACHE_HIT="false"

    if [[ -d "${LOCAL_BTRACKER_CACHE}/datadir" ]]; then
      echo "Local btracker sync cache found at ${LOCAL_BTRACKER_CACHE} - skipping sync"
      CACHE_HIT="true"
      export DATA_SOURCE="${LOCAL_BTRACKER_CACHE}"
    elif [[ -x "$CACHE_MANAGER" ]]; then
      echo "Checking NFS for btracker sync cache: ${BTRACKER_SYNC_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_BTRACKER_CACHE}" 2>/dev/null; then
        echo "Fetched btracker sync cache from NFS - skipping sync"
        CACHE_HIT="true"
        export DATA_SOURCE="${LOCAL_BTRACKER_CACHE}"
      else
        echo "Btracker sync cache not found, will use HAF-only cache and run sync"
      fi
    fi

    # Fall back to HAF-only cache if no btracker cache
    if [[ "$CACHE_HIT" != "true" ]]; then
      if [[ -d "${LOCAL_HAF_CACHE}/datadir" ]]; then
        echo "Local HAF cache found at ${LOCAL_HAF_CACHE}"
      else
        echo "Local HAF cache not found, checking NFS..."
        if [[ -x "$CACHE_MANAGER" ]]; then
          if "$CACHE_MANAGER" get haf "${HAF_COMMIT}" "${LOCAL_HAF_CACHE}"; then
            echo "Fetched HAF replay data from NFS cache"
          else
            echo "ERROR: Failed to fetch HAF replay data from NFS cache"
            exit 1
          fi
        else
          echo "ERROR: cache-manager.sh not found and local cache missing"
          exit 1
        fi
      fi
    fi

    # Export for use in script section
    echo "$CACHE_HIT" > /tmp/cache_hit
  script:
  - |
    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

    # Block log is already in HAF cache from prepare_haf_data, copy_datadir.sh handles it
    "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

    # Using --skip-hived: pre-replayed HAF data from NFS cache contains both
    # PostgreSQL database (pgdata) and shared_memory.bin. HAF just runs PostgreSQL
    # without hived, using the existing replayed data. No need to delete pgdata.

    # Docker Compose bind mounts docker/blockchain over datadir/blockchain.
    # Copy blockchain files there so HAF can see them.
    # Use cp -aL to dereference symlinks (copy actual files, not symlink references)
    # This is needed because datadir/blockchain may contain symlinks to /cache/blockchain/block_log_5m/
    # which isn't accessible inside the Docker Compose containers (DinD environment)
    echo "Copying blockchain files to docker/blockchain..."
    rm -rf "${CI_PROJECT_DIR}/docker/blockchain"/*
    cp -aL "${DATADIR}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Contents of docker/blockchain after copy:"
    ls -la "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Total size:"
    du -sh "${CI_PROJECT_DIR}/docker/blockchain/"

    # Remove blockchain dir from datadir so Docker volume mount doesn't conflict with bind mount
    # Docker Compose will use ./blockchain bind mount exclusively for /home/hived/datadir/blockchain
    sudo rm -rf "${DATADIR}/blockchain"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/start-ci-test-environment.sh"

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"

    if [[ "$CACHE_HIT" == "true" ]]; then
      echo "Cache hit - balance_tracker already synced, skipping wait"
    else
      echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for Balance Tracker to sync..."
      "${CI_PROJECT_DIR}/scripts/ci-helpers/wait-for-bt-startup.sh"
      echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
    fi
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    pushd docker
    IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING

    # Force PostgreSQL checkpoint before shutdown to ensure all btracker data is written to disk
    # Without this, data may be lost when cache-manager excludes WAL files
    echo "Forcing PostgreSQL checkpoint..."
    docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" || true

    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-setup > backend-setup.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > backend-postgrest.log

    # Use longer timeout (60s) for graceful PostgreSQL shutdown
    docker compose "${COMPOSE_OPTIONS[@]}" down --volumes --timeout 60
    popd

    tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log

    # Only save to cache if this was not a cache hit (avoid re-saving what we loaded)
    if [[ "$CACHE_HIT" != "true" ]]; then
      # Save sync data to local cache with commit-based key
      LOCAL_BTRACKER_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
      mkdir -p "${LOCAL_BTRACKER_CACHE}"
      sudo cp -a "${DATADIR}" "${LOCAL_BTRACKER_CACHE}"
      sudo cp -a "${SHM_DIR}" "${LOCAL_BTRACKER_CACHE}"

      # Remove empty blockchain from local cache to trigger symlink creation on test runners
      if [[ -d "${LOCAL_BTRACKER_CACHE}/datadir/blockchain" ]] && [[ -z "$(ls -A "${LOCAL_BTRACKER_CACHE}/datadir/blockchain")" ]]; then
        echo "Removing empty blockchain directory from local cache"
        rmdir "${LOCAL_BTRACKER_CACHE}/datadir/blockchain"
      fi

      ls -lah "${LOCAL_BTRACKER_CACHE}"
      ls -lah "${LOCAL_BTRACKER_CACHE}/datadir" || true
      ls -lah "${LOCAL_BTRACKER_CACHE}/shm_dir" || true

      # Push sync data to NFS cache for sharing across builders
      CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
      if [[ -x "$CACHE_MANAGER" ]]; then
        echo "Pushing btracker sync data to NFS: ${BTRACKER_SYNC_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
        CACHE_HANDLING=haf "$CACHE_MANAGER" put "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_BTRACKER_CACHE}" || echo "Warning: Failed to push to NFS cache"
      else
        echo "Warning: cache-manager.sh not found, skipping NFS cache push"
      fi
    else
      echo "Cache hit - skipping cache save (data already in cache)"
    fi

    # Manually remove the copy of the replay data to preserve disk space on the replay server
    sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

    echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always
  tags:
  - data-cache-storage
  - fast

sync_with_mock_data:
  extends: .docker_image_builder_job_template
  stage: sync
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  - job: docker-ci-runner-build
    artifacts: true
  variables:
    DATA_SOURCE: ${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMPOSE_OPTIONS_STRING: --env-file ci.env --file docker-compose-mocks.yml --file
      overrides/ci.yml --ansi never
  timeout: 1 hours
  before_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
    git config --global --add safe.directory "$CI_PROJECT_DIR"
    git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
    echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  - |
    # Check for existing mock cache first (HAF + btracker + mock data already present)
    LOCAL_MOCK_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    LOCAL_BTRACKER_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    CACHE_HIT="false"

    # First check for mock cache (already has mock data)
    if [[ -d "${LOCAL_MOCK_CACHE}/datadir" ]]; then
      echo "Local mock cache found at ${LOCAL_MOCK_CACHE} - skipping mock fill"
      CACHE_HIT="true"
      export DATA_SOURCE="${LOCAL_MOCK_CACHE}"
    elif [[ -x "$CACHE_MANAGER" ]]; then
      echo "Checking NFS for mock cache: ${BTRACKER_MOCK_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_MOCK_CACHE}" 2>/dev/null; then
        echo "Fetched mock cache from NFS - skipping mock fill"
        CACHE_HIT="true"
        export DATA_SOURCE="${LOCAL_MOCK_CACHE}"
      else
        echo "Mock cache not found, will use btracker sync cache and fill mock data"
      fi
    fi

    # Fall back to btracker sync cache if no mock cache
    if [[ "$CACHE_HIT" != "true" ]]; then
      # Check if local btracker cache exists and is usable
      LOCAL_CACHE_VALID=false
      if [[ -d "${LOCAL_BTRACKER_CACHE}/datadir" ]]; then
        # Check for broken symlinks in pg_tblspc
        TBLSPC="${LOCAL_BTRACKER_CACHE}/datadir/haf_db_store/pgdata/pg_tblspc"
        if [[ -d "$TBLSPC" ]]; then
          BROKEN_LINKS=0
          shopt -s nullglob
          for link in "$TBLSPC"/*; do
            if [[ -L "$link" ]] && [[ ! -e "$link" ]]; then
              BROKEN_LINKS=$((BROKEN_LINKS + 1))
            fi
          done
          shopt -u nullglob
          if [[ "$BROKEN_LINKS" -gt 0 ]]; then
            echo "Local cache has $BROKEN_LINKS broken symlinks, will re-extract"
            sudo rm -rf "${LOCAL_BTRACKER_CACHE}" || rm -rf "${LOCAL_BTRACKER_CACHE}" || true
          else
            LOCAL_CACHE_VALID=true
          fi
        else
          LOCAL_CACHE_VALID=true
        fi
      fi

      if [[ "$LOCAL_CACHE_VALID" == "true" ]]; then
        echo "Local btracker sync cache found at ${LOCAL_BTRACKER_CACHE}"
      else
        echo "Local btracker cache not found or invalid, fetching from NFS..."
        if [[ -x "$CACHE_MANAGER" ]]; then
          if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_BTRACKER_CACHE}"; then
            echo "Fetched btracker sync data from NFS via cache-manager"
          else
            echo "ERROR: Failed to fetch btracker sync data from NFS"
            exit 1
          fi
        else
          echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
          exit 1
        fi
      fi
    fi

    # Export for use in script section
    echo "$CACHE_HIT" > /tmp/cache_hit
  script:
  - |
    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    # If we have a cache hit, the mock data already exists - skip docker-compose entirely
    if [[ "$CACHE_HIT" == "true" ]]; then
      echo "Cache hit - mock data already present in cache, skipping docker-compose"
      echo "Mock cache verified at: ${DATA_SOURCE}"
      exit 0
    fi

    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

    # Block log is already in cache, copy_datadir.sh handles it
    "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

    # Restore PostgreSQL pgdata permissions after copy from cache
    # The cache has relaxed permissions (a+rX) for copying, but PostgreSQL requires mode 700
    PGDATA_PATH="${DATADIR}/haf_db_store/pgdata"
    if [[ -d "$PGDATA_PATH" ]]; then
      echo "Restoring pgdata permissions to mode 700"
      sudo chmod 700 "$PGDATA_PATH"
      sudo chown -R 105:105 "${DATADIR}/haf_db_store"
      ls -la "${DATADIR}/haf_db_store/"
    else
      echo "WARNING: pgdata directory not found at $PGDATA_PATH"
      ls -la "${DATADIR}/" || true
      ls -la "${DATADIR}/haf_db_store/" || true
    fi

    # Docker Compose bind mounts docker/blockchain over datadir/blockchain.
    # Copy blockchain files there so HAF can see them.
    echo "Copying blockchain files to docker/blockchain..."
    rm -rf "${CI_PROJECT_DIR}/docker/blockchain"/*
    cp -aL "${DATADIR}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Contents of docker/blockchain after copy:"
    ls -la "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Total size:"
    du -sh "${CI_PROJECT_DIR}/docker/blockchain/"

    # Remove blockchain dir from datadir so Docker volume mount doesn't conflict with bind mount
    sudo rm -rf "${DATADIR}/blockchain"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/start-ci-test-environment.sh"

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"

    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for mock data fill..."
    "${CI_PROJECT_DIR}/scripts/ci-helpers/wait-for-bt-startup.sh"
    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    # If cache hit, we skipped docker-compose entirely - nothing to clean up
    if [[ "$CACHE_HIT" == "true" ]]; then
      echo "Cache hit - no containers to stop (docker-compose was skipped)"
      echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
      exit 0
    fi

    pushd docker
    IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING

    # Force PostgreSQL checkpoint before shutdown
    echo "Forcing PostgreSQL checkpoint..."
    docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" || true

    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs fill-db-with-mock-data > fill-db-with-mock-data.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log

    # Use longer timeout (60s) for graceful PostgreSQL shutdown
    docker compose "${COMPOSE_OPTIONS[@]}" down --volumes --timeout 60
    popd

    tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log

    # Save mock data to cache
    LOCAL_MOCK_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    mkdir -p "${LOCAL_MOCK_CACHE}"
    sudo cp -a "${DATADIR}" "${LOCAL_MOCK_CACHE}"
    sudo cp -a "${SHM_DIR}" "${LOCAL_MOCK_CACHE}"

    # Remove empty blockchain from local cache
    if [[ -d "${LOCAL_MOCK_CACHE}/datadir/blockchain" ]] && [[ -z "$(ls -A "${LOCAL_MOCK_CACHE}/datadir/blockchain")" ]]; then
      echo "Removing empty blockchain directory from local cache"
      rmdir "${LOCAL_MOCK_CACHE}/datadir/blockchain"
    fi

    ls -lah "${LOCAL_MOCK_CACHE}"
    ls -lah "${LOCAL_MOCK_CACHE}/datadir" || true
    ls -lah "${LOCAL_MOCK_CACHE}/shm_dir" || true

    # Push mock data to NFS cache for sharing across builders
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    if [[ -x "$CACHE_MANAGER" ]]; then
      echo "Pushing mock data to NFS: ${BTRACKER_MOCK_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
      CACHE_HANDLING=haf "$CACHE_MANAGER" put "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_MOCK_CACHE}" || echo "Warning: Failed to push to NFS cache"
    else
      echo "Warning: cache-manager.sh not found, skipping NFS cache push"
    fi

    # Manually remove the copy of the replay data to preserve disk space
    sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

    echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always
  tags:
  - data-cache-storage
  - fast

# HAF instance with NFS fallback for sync data
# Services start before before_script, so we wait for job to extract data if needed
.haf-instance-with-nfs-fallback: &haf-instance-with-nfs-fallback
  name: ${HAF_IMAGE_NAME}
  alias: haf-instance
  variables:
    PGCTLTIMEOUT: 600
    PG_ACCESS: |
      "host    all              haf_admin        0.0.0.0/0    trust"
      "host    all              hived            0.0.0.0/0    trust"
      "host    all              btracker_user       0.0.0.0/0    trust"
      "host    all              btracker_owner      0.0.0.0/0    trust"
      "host    all              all              0.0.0.0/0    scram-sha-256"
    DATA_SOURCE: "/cache/${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
  entrypoint:
    - '/bin/bash'
    - '-c'
    - |
      set -xeuo pipefail
      echo "Service container starting..."
      DATA_PATH="${DATA_SOURCE}"
      MARKER_FILE="${DATA_PATH}/.ready"

      echo "DEBUG: DATA_PATH=${DATA_PATH}"
      echo "DEBUG: MARKER_FILE=${MARKER_FILE}"

      # Wait for job to extract cache via cache-manager (single source of truth for extraction)
      # Timeout matches GitLab Runner's service health check timeout (300s)
      TIMEOUT=300
      WAITED=0
      while [[ ! -f "$MARKER_FILE" ]] && [[ $WAITED -lt $TIMEOUT ]]; do
        # Also check if datadir exists (job may have extracted but marker write failed)
        if [[ -d "${DATA_PATH}/datadir" ]]; then
          echo "Data directory found - proceeding without marker"
          break
        fi
        echo "Waiting for job extraction... ($WAITED/${TIMEOUT}s)"
        sleep 5
        WAITED=$((WAITED + 5))
      done

      if [[ -f "$MARKER_FILE" ]]; then
        echo "Marker file found - using job-extracted data"
      elif [[ -d "${DATA_PATH}/datadir" ]]; then
        echo "Using existing local cache data"
      else
        echo "ERROR: No data available after ${TIMEOUT}s"
        echo "  - Marker file: ${MARKER_FILE} (not found)"
        echo "  - Local cache: ${DATA_PATH}/datadir (not found)"
        echo "Job must extract cache using cache-manager before service can start"
        exit 1
      fi

      echo "Contents of ${DATA_PATH}:"
      ls -la "${DATA_PATH}/" 2>/dev/null | head -10

      # Verify datadir exists
      if [[ ! -d "${DATA_PATH}/datadir" ]]; then
        echo "ERROR: datadir not found at ${DATA_PATH}/datadir"
        exit 1
      fi

      export DATA_SOURCE="${DATA_PATH}"

      # Restore pgdata permissions (PostgreSQL requires 700 or 750)
      PGDATA="${DATA_SOURCE}/datadir/haf_db_store/pgdata"
      if [[ -d "$PGDATA" ]]; then
        echo "Restoring pgdata permissions to mode 700"
        chmod 700 "$PGDATA" 2>/dev/null || sudo chmod 700 "$PGDATA" || true
      fi

      # Run original entrypoint
      exec /home/haf_admin/docker_entrypoint.sh "$@"
    - '/bin/bash'
  command: ["--execute-maintenance-script=${HAF_SOURCE_DIR}/scripts/maintenance-scripts/sleep_infinity.sh"]

# HAF instance with NFS fallback for mock sync data
.haf-instance-with-nfs-fallback-mock: &haf-instance-with-nfs-fallback-mock
  name: ${HAF_IMAGE_NAME}
  alias: haf-instance
  variables:
    PGCTLTIMEOUT: 600
    PG_ACCESS: |
      "host    all              haf_admin        0.0.0.0/0    trust"
      "host    all              hived            0.0.0.0/0    trust"
      "host    all              btracker_user       0.0.0.0/0    trust"
      "host    all              btracker_owner      0.0.0.0/0    trust"
      "host    all              all              0.0.0.0/0    scram-sha-256"
    DATA_SOURCE: "/cache/${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
  entrypoint:
    - '/bin/bash'
    - '-c'
    - |
      set -xeuo pipefail
      echo "Service container starting..."
      DATA_PATH="${DATA_SOURCE}"
      MARKER_FILE="${DATA_PATH}/.ready"

      echo "DEBUG: DATA_PATH=${DATA_PATH}"
      echo "DEBUG: MARKER_FILE=${MARKER_FILE}"

      # Wait for job to extract cache via cache-manager (single source of truth for extraction)
      # Timeout matches GitLab Runner's service health check timeout (300s)
      TIMEOUT=300
      WAITED=0
      while [[ ! -f "$MARKER_FILE" ]] && [[ $WAITED -lt $TIMEOUT ]]; do
        # Also check if datadir exists (job may have extracted but marker write failed)
        if [[ -d "${DATA_PATH}/datadir" ]]; then
          echo "Data directory found - proceeding without marker"
          break
        fi
        echo "Waiting for job extraction... ($WAITED/${TIMEOUT}s)"
        sleep 5
        WAITED=$((WAITED + 5))
      done

      if [[ -f "$MARKER_FILE" ]]; then
        echo "Marker file found - using job-extracted data"
      elif [[ -d "${DATA_PATH}/datadir" ]]; then
        echo "Using existing local cache data"
      else
        echo "ERROR: No data available after ${TIMEOUT}s"
        echo "  - Marker file: ${MARKER_FILE} (not found)"
        echo "  - Local cache: ${DATA_PATH}/datadir (not found)"
        echo "Job must extract cache using cache-manager before service can start"
        exit 1
      fi

      echo "Contents of ${DATA_PATH}:"
      ls -la "${DATA_PATH}/" 2>/dev/null | head -10

      # Verify datadir exists
      if [[ ! -d "${DATA_PATH}/datadir" ]]; then
        echo "ERROR: datadir not found at ${DATA_PATH}/datadir"
        exit 1
      fi

      export DATA_SOURCE="${DATA_PATH}"

      # Restore pgdata permissions (PostgreSQL requires 700 or 750)
      PGDATA="${DATA_SOURCE}/datadir/haf_db_store/pgdata"
      if [[ -d "$PGDATA" ]]; then
        echo "Restoring pgdata permissions to mode 700"
        chmod 700 "$PGDATA" 2>/dev/null || sudo chmod 700 "$PGDATA" || true
      fi

      # Run original entrypoint
      exec /home/haf_admin/docker_entrypoint.sh "$@"
    - '/bin/bash'
  command: ["--execute-maintenance-script=${HAF_SOURCE_DIR}/scripts/maintenance-scripts/sleep_infinity.sh"]

.postgrest-service: &postgrest-service
  name: registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
  alias: postgrest-server
  variables:
    PGRST_ADMIN_SERVER_PORT: 3001
    PGRST_SERVER_PORT: 3000
    # Pointing to the PostgreSQL service running in haf-instance
    PGRST_DB_URI: postgresql://haf_admin@haf-instance:5432/haf_block_log
    PGRST_DB_SCHEMA: btracker_endpoints
    PGRST_DB_ANON_ROLE: btracker_user
    PGRST_DB_POOL: 20
    PGRST_DB_POOL_ACQUISITION_TIMEOUT: 10
    PGRST_DB_EXTRA_SEARCH_PATH: btracker_app
    HEALTHCHECK_TCP_PORT: 3000

# =============================================================================
# Shared before_script templates for test jobs
# =============================================================================
# These templates use scripts/ci-helpers/extract-cache-and-wait.sh to handle:
# - Cache extraction via cache-manager (with race condition handling)
# - PostgreSQL service readiness wait
#
# The helper script is parameterized by cache type and key, reducing duplication.

# Extract sync cache and wait for PostgreSQL
.extract-nfs-cache-before-script: &extract-nfs-cache-before-script
  - $CI_PROJECT_DIR/scripts/ci-helpers/extract-cache-and-wait.sh $BTRACKER_SYNC_CACHE_TYPE $BTRACKER_CACHE_KEY

# Extract sync cache, wait for PostgreSQL, and setup pytest
.extract-nfs-cache-pytest-before-script: &extract-nfs-cache-pytest-before-script
  - $CI_PROJECT_DIR/scripts/ci-helpers/extract-cache-and-wait.sh $BTRACKER_SYNC_CACHE_TYPE $BTRACKER_CACHE_KEY
  - python3 -m venv venv/
  - . venv/bin/activate
  - echo "Entering ${POETRY_INSTALL_ROOT_DIR}"
  - cd "${POETRY_INSTALL_ROOT_DIR}"
  - poetry install

# Extract mock cache and wait for PostgreSQL
.extract-nfs-cache-mock-before-script: &extract-nfs-cache-mock-before-script
  - $CI_PROJECT_DIR/scripts/ci-helpers/extract-cache-and-wait.sh $BTRACKER_MOCK_CACHE_TYPE $BTRACKER_CACHE_KEY

# Extract mock cache, wait for PostgreSQL, and setup pytest
.extract-nfs-cache-mock-pytest-before-script: &extract-nfs-cache-mock-pytest-before-script
  - $CI_PROJECT_DIR/scripts/ci-helpers/extract-cache-and-wait.sh $BTRACKER_MOCK_CACHE_TYPE $BTRACKER_CACHE_KEY
  - python3 -m venv venv/
  - . venv/bin/activate
  - echo "Entering ${POETRY_INSTALL_ROOT_DIR}"
  - cd "${POETRY_INSTALL_ROOT_DIR}"
  - poetry install

# regression-test uses Docker-in-Docker to avoid service container race conditions
# This ensures NFS data is extracted BEFORE containers start
regression-test:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  variables:
    HAF_DATA_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    HAF_SHM_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    COMPOSE_OPTIONS_STRING: --file docker-compose-test.yml --ansi never
  timeout: 30m
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting NFS cache..."

    JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"

    echo "DEBUG: JOB_DIR=${JOB_DIR}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_SYNC_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"

    # Use cache-manager to get data (handles local cache check + NFS tar extraction)
    mkdir -p "${JOB_DIR}"
    if [[ -x "$CACHE_MANAGER" ]]; then
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${JOB_DIR}"; then
        echo "Cache extraction complete"
      else
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi
    else
      echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
      exit 1
    fi

    # Handle blockchain symlinks - dereference and copy to docker dir
    if [[ -L "${HAF_DATA_DIRECTORY}/blockchain" ]] || [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]]; then
      echo "Copying blockchain data to docker directory..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true
      rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
    fi

    ls -la "${HAF_DATA_DIRECTORY}/"
    ls -la "${HAF_SHM_DIRECTORY}/" || true

    echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting test environment..."

    cd "${CI_PROJECT_DIR}/docker"

    # Create ci.env for docker-compose
    cat <<-EOF | tee ci.env
    HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
    HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
    HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
    HIVED_UID=$(id -u)
    POSTGREST_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
    EOF

    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services to be ready..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Wait for HAF PostgreSQL to be healthy
    TIMEOUT=300
    ELAPSED=0
    until pg_isready -h docker -p 5432 -U haf_admin -d haf_block_log 2>/dev/null; do
      sleep 1
      ELAPSED=$((ELAPSED + 1))
      if [ $ELAPSED -ge $TIMEOUT ]; then
        echo "Timeout waiting for PostgreSQL"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs
        exit 1
      fi
    done
    echo "PostgreSQL is ready!"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    cd "${CI_PROJECT_DIR}/tests/account_balances"
    # In DinD, PostgreSQL is available at 'docker' host
    ./accounts_dump_test.sh --host=docker

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up test environment..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Capture container logs before cleanup
    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true

    # Stop and remove containers
    docker compose "${COMPOSE_OPTIONS[@]}" down -v --remove-orphans || true

    # Archive logs
    tar -czvf container-logs.tar.gz *.log 2>/dev/null || true

    echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"
  artifacts:
    paths:
    - tests/account_balances/account_dump_test.log
    - docker/container-logs.tar.gz
    when: always
  tags:
  - data-cache-storage

setup-scripts-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    cd tests/functional
    ./test_scripts.sh --host=haf-instance

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  tags:
  - data-cache-storage
  - fast

performance-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  - *postgrest-service
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    # Wait for PostgREST to be ready (healthcheck on port 3000)
    echo "Waiting for PostgREST service..."
    WAIT_TIMEOUT=120
    WAIT_INTERVAL=2
    WAITED=0
    while ! curl -sf http://postgrest-server:3000/ > /dev/null 2>&1; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for PostgREST after ${WAIT_TIMEOUT}s"
        echo "Checking if service is reachable..."
        curl -v http://postgrest-server:3000/ || true
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgREST... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgREST is ready!"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    timeout -k 1m 10m ./balance-tracker.sh run-tests --backend-host=postgrest-server --postgres-host=haf-instance
    tar -czvf tests/performance/results.tar.gz $(pwd)/tests/performance/*result.*
    cat jmeter.log | python3 docker/ci/parse-jmeter-output.py
    m2u --input $(pwd)/tests/performance/result.xml --output $(pwd)/tests/performance/junit-result.xml

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    - tests/performance/result_report/
    - tests/performance/results.tar.gz
    - jmeter.log
    reports:
      junit: tests/performance/junit-result.xml
  tags:
  - data-cache-storage
  - fast

# pattern-test uses Docker-in-Docker to avoid service container race conditions
# This ensures NFS data is extracted BEFORE containers start
pattern-test:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  variables:
    HAF_DATA_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    HAF_SHM_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    COMPOSE_OPTIONS_STRING: --file docker-compose-test.yml --ansi never
    JUNIT_REPORT: $CI_PROJECT_DIR/tests/tavern/report.xml
    BTRACKER_ADDRESS: localhost
    BTRACKER_PORT: 3000
    TAVERN_DIR: $CI_PROJECT_DIR/tests/tavern
    PYTEST_NUMBER_OF_PROCESSES: 16
  timeout: 30m
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting NFS cache..."

    JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"

    echo "DEBUG: JOB_DIR=${JOB_DIR}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_SYNC_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"

    # Use cache-manager to get data (handles local cache check + NFS tar extraction)
    mkdir -p "${JOB_DIR}"
    if [[ -x "$CACHE_MANAGER" ]]; then
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${JOB_DIR}"; then
        echo "Cache extraction complete"
      else
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi
    else
      echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
      exit 1
    fi

    # Handle blockchain symlinks - dereference and copy to docker dir
    if [[ -L "${HAF_DATA_DIRECTORY}/blockchain" ]] || [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]]; then
      echo "Copying blockchain data to docker directory..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true
      rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
    fi

    ls -la "${HAF_DATA_DIRECTORY}/"
    ls -la "${HAF_SHM_DIRECTORY}/" || true

    echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting test environment..."

    cd "${CI_PROJECT_DIR}/docker"

    # Create ci.env for docker-compose
    cat <<-EOF | tee ci.env
    HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
    HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
    HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
    HIVED_UID=$(id -u)
    POSTGREST_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
    EOF

    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services to be ready..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Wait for HAF PostgreSQL to be healthy
    WAITED=0
    MAX_WAIT=300
    until docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" exec -T haf pg_isready -U haf_admin -d haf_block_log 2>/dev/null; do
      echo "Waiting for PostgreSQL... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgreSQL did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf | tail -50
        exit 1
      fi
    done
    echo "PostgreSQL is ready!"

    # Wait for PostgREST to be ready
    # In DinD, exposed ports are available at 'docker' host, not localhost
    WAITED=0
    until curl -s http://docker:3000/ > /dev/null 2>&1; do
      echo "Waiting for PostgREST at docker:3000... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgREST did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest | tail -50
        exit 1
      fi
    done
    echo "PostgREST is ready at docker:3000!"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):venv[collapsed=true]\r\e[0KSetting up Python environment..."

    # Install test dependencies directly (avoid full hive-local-tools which has incompatible hiveio-wax wheel)
    python3 -m venv venv/
    . venv/bin/activate

    echo "Installing test dependencies..."
    pip install tavern==2.2.0 pytest-xdist==3.2.0 pyyaml requests deepdiff prettytable
    # Install tests_api for validate_response module (required by tavern tests)
    pip install -e "${CI_PROJECT_DIR}/haf/hive/tests/python/hive-local-tools/tests_api"

    echo -e "\e[0Ksection_end:$(date +%s):venv\r\e[0K"
  script:
  - |
    . venv/bin/activate
    cd $CI_PROJECT_DIR/tests/tavern
    # In DinD, PostgREST is available at 'docker' host
    BTRACKER_ADDRESS=docker pytest -n $PYTEST_NUMBER_OF_PROCESSES --junitxml report.xml .
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up test environment..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Capture logs before stopping
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest > postgrest.log 2>&1 || true

    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" down --volumes || true

    tar -czvf container-logs.tar.gz haf.log postgrest.log 2>/dev/null || true

    # Clean up job-specific data directory
    sudo rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || true

    echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"
  artifacts:
    paths:
    - "**/*.out.json"
    - docker/container-logs.tar.gz
    reports:
      junit: tests/tavern/report.xml
    when: always
  tags:
  - data-cache-storage
  - fast

# pattern-test-with-mock-data uses Docker-in-Docker to avoid service container race conditions
# This ensures NFS data is extracted BEFORE containers start
pattern-test-with-mock-data:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync_with_mock_data
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  variables:
    HAF_DATA_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    HAF_SHM_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    COMPOSE_OPTIONS_STRING: --file docker-compose-test.yml --ansi never
    JUNIT_REPORT: $CI_PROJECT_DIR/tests/tavern_mocks/report.xml
    BTRACKER_ADDRESS: localhost
    BTRACKER_PORT: 3000
    TAVERN_DIR: $CI_PROJECT_DIR/tests/tavern_mocks
    PYTEST_NUMBER_OF_PROCESSES: 16
  timeout: 30m
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting NFS cache (mock data)..."

    JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"

    echo "DEBUG: JOB_DIR=${JOB_DIR}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_MOCK_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"

    # Use cache-manager to get data (handles local cache check + NFS tar extraction)
    mkdir -p "${JOB_DIR}"
    if [[ -x "$CACHE_MANAGER" ]]; then
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${JOB_DIR}"; then
        echo "Cache extraction complete"
      else
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi
    else
      echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
      exit 1
    fi

    # Handle blockchain symlinks - dereference and copy to docker dir
    if [[ -L "${HAF_DATA_DIRECTORY}/blockchain" ]] || [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]]; then
      echo "Copying blockchain data to docker directory..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true
      rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
    fi

    ls -la "${HAF_DATA_DIRECTORY}/"
    ls -la "${HAF_SHM_DIRECTORY}/" || true

    echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting test environment..."

    cd "${CI_PROJECT_DIR}/docker"

    # Create ci.env for docker-compose
    cat <<-EOF | tee ci.env
    HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
    HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
    HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
    HIVED_UID=$(id -u)
    POSTGREST_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
    EOF

    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services to be ready..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Wait for HAF PostgreSQL to be healthy
    WAITED=0
    MAX_WAIT=300
    until docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" exec -T haf pg_isready -U haf_admin -d haf_block_log 2>/dev/null; do
      echo "Waiting for PostgreSQL... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgreSQL did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf | tail -50
        exit 1
      fi
    done
    echo "PostgreSQL is ready!"

    # Wait for PostgREST to be ready
    # In DinD, exposed ports are available at 'docker' host, not localhost
    WAITED=0
    until curl -s http://docker:3000/ > /dev/null 2>&1; do
      echo "Waiting for PostgREST at docker:3000... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgREST did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest | tail -50
        exit 1
      fi
    done
    echo "PostgREST is ready at docker:3000!"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):venv[collapsed=true]\r\e[0KSetting up Python environment..."

    # Install test dependencies directly (avoid full hive-local-tools which has incompatible hiveio-wax wheel)
    python3 -m venv venv/
    . venv/bin/activate

    echo "Installing test dependencies..."
    pip install tavern==2.2.0 pytest-xdist==3.2.0 pyyaml requests deepdiff prettytable
    # Install tests_api for validate_response module (required by tavern tests)
    pip install -e "${CI_PROJECT_DIR}/haf/hive/tests/python/hive-local-tools/tests_api"

    echo -e "\e[0Ksection_end:$(date +%s):venv\r\e[0K"
  script:
  - |
    . venv/bin/activate
    cd $CI_PROJECT_DIR/tests/tavern_mocks
    # In DinD, PostgREST is available at 'docker' host
    BTRACKER_ADDRESS=docker pytest -n $PYTEST_NUMBER_OF_PROCESSES --junitxml report.xml .
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up test environment..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Capture logs before stopping
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest > postgrest.log 2>&1 || true

    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" down --volumes || true

    tar -czvf container-logs.tar.gz haf.log postgrest.log 2>/dev/null || true

    # Clean up job-specific data directory
    sudo rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || true

    echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"
  artifacts:
    paths:
    - "**/*.out.json"
    - docker/container-logs.tar.gz
    reports:
      junit: tests/tavern_mocks/report.xml
    when: always
  tags:
  - data-cache-storage
  - fast

python_api_client_test:
  extends: .project_develop_configuration_template
  stage: test
  needs:
  - job: generate_python_api_client
    artifacts: true
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  script:
  - pytest "${PYPROJECT_DIR}/tests"
  tags:
  - public-runner-docker

build_and_publish_image:
  stage: publish
  extends: .publish_docker_image_template
  before_script:
  - !reference [.publish_docker_image_template, before_script]
  script:
  - |
    scripts/ci-helpers/build_and_publish_instance.sh
    if [[ -n "$CI_COMMIT_TAG" ]]; then
      docker tag "$CI_REGISTRY_IMAGE/postgrest-rewriter:$CI_COMMIT_TAG" "registry-upload.hive.blog/balance_tracker/postgrest-rewriter:$CI_COMMIT_TAG"
      docker push "registry-upload.hive.blog/balance_tracker/postgrest-rewriter:$CI_COMMIT_TAG"
    fi
  tags:
  - public-runner-docker
  - build-mainnet

deploy_python_api_packages_to_gitlab:
  stage: publish
  needs:
  - job: build_python_api_client_wheel
    artifacts: true
  - job: generate_python_api_client
    artifacts: true
  - job: python_api_client_test
  extends: .deploy_wheel_to_gitlab_template
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  when: on_success
  tags:
  - public-runner-docker

deploy-wax-spec-dev-package:
  extends: .npm_deploy_package_template
  stage: publish
  variables:
    # Use artifact paths directly - dotenv vars contain absolute paths from previous
    # job's runner which don't exist on this runner
    SOURCE_DIR: "${CI_PROJECT_DIR}/build/generated"
    NPM_PACKAGE_SCOPE: "@hiveio"
  before_script:
    - git config --global --add safe.directory '*'
    - . "${NVM_DIR}/nvm.sh"
    - nvm use "${NODEJS_VERSION}"
    - cd "${SOURCE_DIR}"
    - pnpm --recursive install --frozen-lockfile
    # Find the tgz file - filename includes version number so we can't hardcode it
    - export PACKAGE_TGZ_PATH=$(find "${CI_PROJECT_DIR}/build" -name "*.tgz" | head -1)
    - echo "Found package at ${PACKAGE_TGZ_PATH}"
    - echo -e "\e[0Ksection_start:$(date +%s):package_tgz_unpack[collapsed=true]\r\e[0KAttempting to unpack ${PACKAGE_TGZ_PATH}"
    - cd "${CI_PROJECT_DIR}"
    - tar -xf "${PACKAGE_TGZ_PATH}" -C "${SOURCE_DIR}" --strip-components=1
    - echo -e "\e[0Ksection_end:$(date +%s):package_tgz_unpack\r\e[0K"
  needs:
  - job: generate-wax-spec
    artifacts: true
  tags:
  - public-runner-docker

deploy-wax-spec-production-public-npm:
  extends: .registry_npmjs_org_deploy_package_template
  stage: publish
  variables:
    NPM_PUBLISH_TOKEN: "$INTERNAL_HIDDEN_PUBLISH_TOKEN"
    NPM_PACKAGE_NAME: "wax-api-balance-tracker"
    # Use artifact paths directly - dotenv vars contain absolute paths from previous
    # job's runner which don't exist on this runner
    SOURCE_DIR: "${CI_PROJECT_DIR}/build/generated"
  before_script:
    - git config --global --add safe.directory '*'
    - . "${NVM_DIR}/nvm.sh"
    - nvm use "${NODEJS_VERSION}"
    - cd "${SOURCE_DIR}"
    - pnpm --recursive install --frozen-lockfile
    # Find the tgz file - filename includes version number so we can't hardcode it
    - export PACKAGE_TGZ_PATH=$(find "${CI_PROJECT_DIR}/build" -name "*.tgz" | head -1)
    - echo "Found package at ${PACKAGE_TGZ_PATH}"
    - echo -e "\e[0Ksection_start:$(date +%s):package_tgz_unpack[collapsed=true]\r\e[0KAttempting to unpack ${PACKAGE_TGZ_PATH}"
    - cd "${CI_PROJECT_DIR}"
    - tar -xf "${PACKAGE_TGZ_PATH}" -C "${SOURCE_DIR}" --strip-components=1
    - echo -e "\e[0Ksection_end:$(date +%s):package_tgz_unpack\r\e[0K"
  needs:
  - job: generate-wax-spec
    artifacts: true
  tags:
  - public-runner-docker

cleanup_haf_cache_manual:
  extends: .cleanup_cache_manual_template
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "${DATA_CACHE_HAF_PREFIX}_*"
  tags:
  - data-cache-storage
  - fast
