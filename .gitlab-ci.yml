stages:
- lint
- build
- sync
- test
- cleanup
- publish

variables:
  # Git configuration
  GIT_STRATEGY: clone
  GIT_SUBMODULE_STRATEGY: recursive
  GIT_DEPTH: 1
  GIT_SUBMODULE_DEPTH: 1
  GIT_SUBMODULE_UPDATE_FLAGS: --jobs 4
  # HAF configuration
  DATA_CACHE_HAF_PREFIX: "/cache/replay_data_haf"
  # NFS cache configuration for sync data sharing across builders
  DATA_CACHE_NFS_PREFIX: "/nfs/ci-cache"
  SYNC_CACHE_KEY: "${HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}"
  SYNC_CACHE_KEY_MOCK: "${HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}_mock"
  SYNC_CACHE_TYPE: "haf_sync"
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m
  FF_NETWORK_PER_BUILD: 1
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu24.04-10
  BUILDER_IMAGE_TAG: "$TEST_HAF_IMAGE_TAG"
  BUILDER_IMAGE_PATH: "registry.gitlab.syncad.com/hive/haf/ci-base-image${BUILDER_IMAGE_TAG}"
  # HAF submodule commit - must match the 'ref:' in the include section below
  # This is needed for service containers which can't access dotenv artifacts
  HAF_COMMIT: "36a34d7a7d8ccd5392379390d4211a59a6ec8d98"

include:
- template: Workflows/Branch-Pipelines.gitlab-ci.yml
- project: hive/haf
  ref: 36a34d7a7d8ccd5392379390d4211a59a6ec8d98   # feature/nfs-cache-manager
  file: /scripts/ci-helpers/prepare_data_image_job.yml
  # Do not include common-ci-configuration here, it is already referenced by scripts/ci-helpers/prepare_data_image_job.yml included from Haf/Hive repos

.lint_job:
  stage: lint
  variables:
    GIT_SUBMODULE_STRATEGY: none
  artifacts:
    name: lint-results
    when: always
  tags:
  - public-runner-docker

lint_bash_scripts:
  extends: .lint_job
  image: koalaman/shellcheck-alpine:latest
  before_script:
  - apk add xmlstarlet
  script:
  - find . -name .git -type d -prune -o -type f -name \*.sh -exec shellcheck -f checkstyle
    {} + | tee shellcheck-checkstyle-result.xml
  after_script:
  - xmlstarlet tr misc/checkstyle2junit.xslt shellcheck-checkstyle-result.xml > shellcheck-junit-result.xml
  artifacts:
    paths:
    - shellcheck-checkstyle-result.xml
    - shellcheck-junit-result.xml
    reports:
      junit: shellcheck-junit-result.xml

lint_sql_scripts:
  extends: .lint_job
  image:
    name: sqlfluff/sqlfluff:2.1.4
    entrypoint: [""]
  script:
  - sqlfluff lint --format yaml --write-output sql-lint.yaml
  artifacts:
    paths:
    - sql-lint.yaml

validate_haf_commit:
  stage: build
  image: alpine:latest
  script:
  - |
    set -e
    apk add --no-cache git
    # Validate that HAF_COMMIT variable matches both the submodule and include ref
    # This prevents cache misses due to mismatched commits
    SUBMODULE_COMMIT=$(cat .git/modules/haf/HEAD 2>/dev/null || git -C haf rev-parse HEAD)
    INCLUDE_REF=$(grep -A2 "project:.*hive/haf" .gitlab-ci.yml | grep "ref:" | head -1 | sed 's/.*ref: *\([a-f0-9]*\).*/\1/' || true)

    echo "HAF_COMMIT variable: $HAF_COMMIT"
    echo "HAF submodule HEAD:  $SUBMODULE_COMMIT"
    echo "Include ref:         $INCLUDE_REF"

    ERRORS=0
    if [ "$HAF_COMMIT" != "$SUBMODULE_COMMIT" ]; then
      echo "ERROR: HAF_COMMIT variable does not match submodule commit!"
      echo "       Update HAF_COMMIT in .gitlab-ci.yml to: $SUBMODULE_COMMIT"
      ERRORS=1
    fi
    if [ "$HAF_COMMIT" != "$INCLUDE_REF" ]; then
      echo "ERROR: HAF_COMMIT variable does not match include ref!"
      echo "       Both should be: $HAF_COMMIT"
      ERRORS=1
    fi
    if [ $ERRORS -eq 1 ]; then
      echo ""
      echo "To fix: ensure HAF_COMMIT, include ref, and submodule all use the same commit"
      exit 1
    fi
    echo "All HAF commit references are consistent"
  tags:
  - public-runner-docker

prepare_haf_image:
  stage: build
  extends: .prepare_haf_image
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$HAF_DEPLOY_USERNAME"
    REGISTRY_PASS: "$HAF_DEPLOY_TOKEN"
  before_script:
  - git config --global --add safe.directory $CI_PROJECT_DIR/haf
  tags:
  - public-runner-docker
  - build-mainnet

extract-swagger-json:
  extends: .filter_out_swagger_json
  stage: build
  variables:
    INPUT_SQL_SWAGGER_FILE: "${CI_PROJECT_DIR}/endpoints/endpoint_schema.sql"
  tags:
  - public-runner-docker

generate_python_api_client:
  extends: .project_develop_configuration_template
  stage: build
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  needs:
  - job: extract-swagger-json
    artifacts: true
  script:
  - ${PYPROJECT_DIR}/generate_balance_tracker_api_client.sh
  artifacts:
    paths:
    - "${PYPROJECT_DIR}/balance_api"
    - "${PYPROJECT_DIR}/balance_api/balance_api_client"
  tags:
  - public-runner-docker

build_python_api_client_wheel:
  extends: .build_wheel_template
  stage: build
  needs:
  - job: generate_python_api_client
    artifacts: true
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  tags:
  - public-runner-docker

generate-wax-spec:
  extends: .generate_swagger_package
  stage: build
  variables:
    INPUT_JSON_SWAGGER_FILE: "${BUILT_JSON_SWAGGER_FILE}"
    API_TYPE: "rest"
    NAMESPACE: "balance_tracker"
    NPM_PACKAGE_SCOPE: "@hiveio"
    NPM_PACKAGE_NAME: "wax-api-balance-tracker"
  needs:
  - job: extract-swagger-json
    artifacts: true
  tags:
  - public-runner-docker

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
  - job: prepare_haf_image
    artifacts: true
  stage: build
  timeout: 80m
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/docker/config_5M.ini"
  tags:
  - data-cache-storage
  - fast

.docker-build-template:
  extends: .docker_image_builder_job_template
  stage: build
  variables:
    BASE_REPO_NAME: ""
    BASE_TAG: ""
    NAME: ""
    TARGET: "$NAME"
    PROGRESS_DISPLAY: "plain"
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  script:
  - |
    echo -e "\e[0Ksection_end:$(date +%s):tag\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):build[collapsed=true]\r\e[0KBaking $NAME${BASE_REPO_NAME:+/$BASE_REPO_NAME} image..."
    function image-exists() {
      local image=$1
      docker manifest inspect "$1" > /dev/null
      return $?
    }
    if image-exists "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:$BASE_TAG"; then
      echo "Image $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG} already exists. Skipping..."
      if [[ -n "$CI_COMMIT_TAG" && "$TARGET" == "full-ci" ]]; then
        echo "Tagging pre-existing image with Git tag..."
        docker pull "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG}"
        docker tag "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG}" "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
        docker push "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
      fi
    else
      echo "Baking $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG} image..."
      git config --global --add safe.directory $(pwd)
      scripts/ci-helpers/build_docker_image.sh "$CI_PROJECT_DIR"
    fi
    echo -e "\e[0Ksection_end:$(date +%s):build\r\e[0K"
  tags:
  - public-runner-docker
  - build-mainnet

docker-ci-runner-build:
  extends: .docker-build-template
  variables:
    BASE_REPO_NAME: ""
    BASE_TAG: "docker-24.0.1-11"
    NAME: "ci-runner"
    TARGET: "ci-runner-ci"

docker-setup-docker-image-build:
  extends: .docker-build-template
  variables:
    GIT_SUBMODULE_STRATEGY: none
    GIT_DEPTH: 1
    BASE_REPO_NAME: ""
    BASE_TAG: "$CI_COMMIT_SHORT_SHA"
    NAME: ""
    TARGET: "full-ci"

sync:
  extends: .docker_image_builder_job_template
  stage: sync
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-11
  needs:
  - prepare_haf_image
  - prepare_haf_data
  - docker-setup-docker-image-build
  - docker-ci-runner-build
  variables:
    DATA_SOURCE: ${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    SHARED_BLOCK_LOG_DIR: ${BLOCK_LOG_SOURCE_DIR_5M}
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMPOSE_OPTIONS_STRING: --env-file ci.env --file docker-compose.yml --file overrides/ci.yml
      --ansi never
  timeout: 1 hours
  before_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
    git config --global --add safe.directory "$CI_PROJECT_DIR"
    git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
    echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  - |
    # Ensure HAF replay data is available locally (fetch from NFS if needed)
    LOCAL_HAF_CACHE="${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}"
    if [[ -d "${LOCAL_HAF_CACHE}/datadir" ]]; then
      echo "Local HAF cache found at ${LOCAL_HAF_CACHE}"
    else
      echo "Local HAF cache not found, checking NFS..."
      CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
      if [[ -x "$CACHE_MANAGER" ]]; then
        if "$CACHE_MANAGER" get haf "${HAF_COMMIT}" "${LOCAL_HAF_CACHE}"; then
          echo "Fetched HAF replay data from NFS cache"
        else
          echo "ERROR: Failed to fetch HAF replay data from NFS cache"
          exit 1
        fi
      else
        echo "ERROR: cache-manager.sh not found and local cache missing"
        exit 1
      fi
    fi
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

    # Block log is already in HAF cache from prepare_haf_data, copy_datadir.sh handles it
    "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

    # Remove shared_memory.bin, comments-rocksdb-storage, AND pgdata to force HAF to replay from block 1
    # The replay_data_haf cache includes shared_memory.bin and pgdata at 5M blocks.
    # If we keep shared_memory.bin, hived thinks it's at 5M blocks and skips replay.
    # If we remove only shared_memory.bin but keep pgdata, hived replays and inserts duplicates
    # into the existing PostgreSQL database, causing unique constraint violations.
    # By removing all three, hived will initialize a fresh database and replay all blocks.
    if [[ -f "${SHM_DIR}/shared_memory.bin" ]]; then
      echo "Removing shared_memory.bin, comments-rocksdb-storage, and pgdata to force HAF replay into PostgreSQL"
      rm -f "${SHM_DIR}/shared_memory.bin"
      rm -rf "${SHM_DIR}/comments-rocksdb-storage"
      sudo rm -rf "${DATADIR}/haf_db_store/pgdata"
      sudo rm -rf "${DATADIR}/haf_db_store/tablespace"
      # Fix permissions so container can create pgdata (runs as hived user)
      sudo chmod 777 "${DATADIR}/haf_db_store"
    fi

    # Note: pgdata and tablespace were removed above to force fresh database initialization.
    # HAF entrypoint will run initdb to create a new database.

    # Docker Compose bind mounts docker/blockchain over datadir/blockchain.
    # Copy blockchain files there so HAF can see them.
    # Use cp -aL to dereference symlinks (copy actual files, not symlink references)
    # This is needed because datadir/blockchain may contain symlinks to /cache/blockchain/block_log_5m/
    # which isn't accessible inside the Docker Compose containers (DinD environment)
    echo "Copying blockchain files to docker/blockchain..."
    rm -rf "${CI_PROJECT_DIR}/docker/blockchain"/*
    cp -aL "${DATADIR}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Contents of docker/blockchain after copy:"
    ls -la "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Total size:"
    du -sh "${CI_PROJECT_DIR}/docker/blockchain/"

    # Remove blockchain dir from datadir so Docker volume mount doesn't conflict with bind mount
    # Docker Compose will use ./blockchain bind mount exclusively for /home/hived/datadir/blockchain
    # Note: files may be owned by hived user with restricted permissions from copy_datadir.sh
    sudo chmod -R a+w "${DATADIR}/blockchain" 2>/dev/null || true
    sudo rm -rf "${DATADIR}/blockchain"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/start-ci-test-environment.sh"

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0K$MESSAGE"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/wait-for-bt-startup.sh"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

    pushd docker
    IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-setup > backend-setup.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > backend-postgrest.log

    # Force PostgreSQL checkpoint before stopping for clean cache
    # This ensures pg_controldata reports the correct checkpoint WAL for cache-manager
    echo "Running PostgreSQL CHECKPOINT before shutdown..."
    docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" 2>/dev/null || true

    # Stop containers - PostgreSQL will do a clean shutdown after checkpoint
    docker compose "${COMPOSE_OPTIONS[@]}" down --volumes
    popd

    tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log

    # Save sync data to local cache with commit-based key (not pipeline ID)
    # Use /cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY} to match what test jobs expect
    # Remove old cache first to ensure clean copy (cp -a merges, doesn't replace)
    LOCAL_SYNC_CACHE="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY}"
    sudo rm -rf "${LOCAL_SYNC_CACHE}"
    mkdir -p "${LOCAL_SYNC_CACHE}"
    sudo cp -a "${DATADIR}" "${LOCAL_SYNC_CACHE}"
    sudo cp -a "${SHM_DIR}" "${LOCAL_SYNC_CACHE}"

    ls -lah "${DATADIR}"
    ls -lah "${DATADIR}/blockchain"

    ls -lah "${LOCAL_SYNC_CACHE}"
    ls -lah "${LOCAL_SYNC_CACHE}/datadir" || true
    ls -lah "${LOCAL_SYNC_CACHE}/datadir/blockchain" || true
    ls -lah "${LOCAL_SYNC_CACHE}/shm_dir" || true

    # Push sync data to NFS cache for sharing across builders
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    if [[ -x "$CACHE_MANAGER" ]]; then
      echo "Pushing sync data to NFS cache: ${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY}"
      "$CACHE_MANAGER" put "${SYNC_CACHE_TYPE}" "${SYNC_CACHE_KEY}" "${LOCAL_SYNC_CACHE}" || echo "Warning: Failed to push to NFS cache"
    else
      echo "Warning: cache-manager.sh not found, skipping NFS cache push"
    fi

    # Manually remove the copy of the replay data to preserve disk space on the replay server
    sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

    echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always
  tags:
  - data-cache-storage
  - fast

sync_with_mock_data:
  extends: .docker_image_builder_job_template
  stage: sync
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-11
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  - job: docker-ci-runner-build
    artifacts: true
  variables:
    # DATA_SOURCE must match LOCAL_SYNC_CACHE in before_script where NFS tar is extracted
    DATA_SOURCE: /cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMPOSE_OPTIONS_STRING: --env-file ci.env --file docker-compose-mocks.yml --file
      overrides/ci.yml --ansi never
  timeout: 1 hours
  before_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
    git config --global --add safe.directory "$CI_PROJECT_DIR"
    git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
    echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"

    # Ensure sync data is available locally (fetch from NFS if needed)
    echo "DEBUG: Setting up cache paths..."
    LOCAL_SYNC_CACHE="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    NFS_CACHE_PATH="${DATA_CACHE_NFS_PREFIX}/${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY}"
    NFS_TAR="${NFS_CACHE_PATH}.tar"
    echo "DEBUG: LOCAL_SYNC_CACHE=${LOCAL_SYNC_CACHE}"
    echo "DEBUG: NFS_TAR=${NFS_TAR}"

    # Check if local cache exists and is usable
    # Service container extractions may have broken symlinks, so validate pg_tblspc
    LOCAL_CACHE_VALID=false
    echo "DEBUG: Checking local cache..."
    if [[ -d "${LOCAL_SYNC_CACHE}/datadir" ]]; then
      echo "DEBUG: Local cache datadir exists"
      # Check for broken symlinks in pg_tblspc
      TBLSPC="${LOCAL_SYNC_CACHE}/datadir/haf_db_store/pgdata/pg_tblspc"
      if [[ -d "$TBLSPC" ]]; then
        echo "DEBUG: Checking tablespace symlinks in $TBLSPC"
        # Find broken symlinks portably (find -xtype not available in all find versions)
        BROKEN_LINKS=0
        shopt -s nullglob  # Handle empty directories gracefully
        for link in "$TBLSPC"/*; do
          if [[ -L "$link" ]] && [[ ! -e "$link" ]]; then
            BROKEN_LINKS=$((BROKEN_LINKS + 1))
          fi
        done
        shopt -u nullglob
        if [[ "$BROKEN_LINKS" -gt 0 ]]; then
          echo "Local cache has $BROKEN_LINKS broken symlinks in pg_tblspc, will re-extract"
          sudo rm -rf "${LOCAL_SYNC_CACHE}" || rm -rf "${LOCAL_SYNC_CACHE}" || true
        else
          LOCAL_CACHE_VALID=true
        fi
      else
        echo "DEBUG: No pg_tblspc directory, cache is valid"
        LOCAL_CACHE_VALID=true
      fi
    else
      echo "DEBUG: Local cache datadir does not exist"
    fi

    if [[ "$LOCAL_CACHE_VALID" == "true" ]]; then
      echo "Local sync cache found at ${LOCAL_SYNC_CACHE}"
    else
      echo "Local sync cache not found or invalid, checking NFS..."
      # Prefer NFS tar extraction over cache-manager for haf_sync type
      # (cache-manager cp -rL breaks on tablespace symlinks)
      if [[ -f "$NFS_TAR" ]]; then
        echo "Extracting from NFS tar: $NFS_TAR"
        mkdir -p "${LOCAL_SYNC_CACHE}"
        tar xf "$NFS_TAR" -C "${LOCAL_SYNC_CACHE}"
        echo "Extracted sync data from NFS tar archive"
      elif [[ -x "$CACHE_MANAGER" ]]; then
        if "$CACHE_MANAGER" get "${SYNC_CACHE_TYPE}" "${SYNC_CACHE_KEY}" "${LOCAL_SYNC_CACHE}"; then
          echo "Fetched sync data from NFS cache via cache-manager"
        else
          echo "ERROR: Failed to fetch sync data from NFS cache"
          exit 1
        fi
      elif [[ -d "$NFS_CACHE_PATH" ]]; then
        echo "Using direct NFS copy (cache-manager not available)"
        mkdir -p "${LOCAL_SYNC_CACHE}"
        # Use tar pipe to preserve symlinks correctly
        (cd "$NFS_CACHE_PATH" && tar cf - .) | (cd "${LOCAL_SYNC_CACHE}" && tar xf -)
        echo "Fetched sync data from NFS cache via tar pipe"
      else
        echo "ERROR: No sync data available locally or on NFS"
        echo "  Local path checked: ${LOCAL_SYNC_CACHE}"
        echo "  NFS tar checked: ${NFS_TAR}"
        echo "  NFS path checked: ${NFS_CACHE_PATH}"
        exit 1
      fi
    fi
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

    # Block log is already in sync cache from sync job, copy_datadir.sh handles it
    "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

    # Restore PostgreSQL pgdata permissions after copy from cache
    # The cache has relaxed permissions (a+rX) for copying, but PostgreSQL requires mode 700
    PGDATA_PATH="${DATADIR}/haf_db_store/pgdata"
    if [[ -d "$PGDATA_PATH" ]]; then
      echo "Restoring pgdata permissions to mode 700"
      # pgdata directory itself must be 700
      sudo chmod 700 "$PGDATA_PATH"
      # Set ownership to postgres user (UID 105 in HAF container)
      sudo chown -R 105:105 "${DATADIR}/haf_db_store"
      # Debug: show permissions (don't list inside pgdata - only postgres can read it after chmod 700)
      ls -la "${DATADIR}/haf_db_store/"
    else
      echo "WARNING: pgdata directory not found at $PGDATA_PATH"
      ls -la "${DATADIR}/" || true
      ls -la "${DATADIR}/haf_db_store/" || true
    fi

    # Docker Compose bind mounts docker/blockchain over datadir/blockchain.
    # Copy blockchain files there so HAF can see them.
    # Use cp -aL to dereference symlinks (copy actual files, not symlink references)
    # This is needed because datadir/blockchain may contain symlinks to /cache/blockchain/block_log_5m/
    # which isn't accessible inside the Docker Compose containers (DinD environment)
    echo "Copying blockchain files to docker/blockchain..."
    rm -rf "${CI_PROJECT_DIR}/docker/blockchain"/*
    cp -aL "${DATADIR}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Contents of docker/blockchain after copy:"
    ls -la "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Total size:"
    du -sh "${CI_PROJECT_DIR}/docker/blockchain/"

    # Remove blockchain dir from datadir so Docker volume mount doesn't conflict with bind mount
    # Docker Compose will use ./blockchain bind mount exclusively for /home/hived/datadir/blockchain
    # Note: files may be owned by hived user with restricted permissions from copy_datadir.sh
    sudo chmod -R a+w "${DATADIR}/blockchain" 2>/dev/null || true
    sudo rm -rf "${DATADIR}/blockchain"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/start-ci-test-environment.sh"

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0K$MESSAGE"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/wait-for-bt-startup.sh"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

    pushd docker
    IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs fill-db-with-mock-data > fill-db-with-mock-data.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log

    # Force PostgreSQL checkpoint before stopping for clean cache
    # This ensures pg_controldata reports the correct checkpoint WAL for cache-manager
    echo "Running PostgreSQL CHECKPOINT before shutdown..."
    docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" 2>/dev/null || true

    # Stop containers - PostgreSQL will do a clean shutdown after checkpoint
    docker compose "${COMPOSE_OPTIONS[@]}" down --volumes
    popd

    tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log

    # Save mock sync data to local cache with commit-based key
    # Use /cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY_MOCK} to match what test jobs expect
    # Remove old cache first to ensure clean copy (cp -a merges, doesn't replace)
    LOCAL_SYNC_CACHE_MOCK="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY_MOCK}"
    sudo rm -rf "${LOCAL_SYNC_CACHE_MOCK}"
    mkdir -p "${LOCAL_SYNC_CACHE_MOCK}"
    sudo cp -a "${DATADIR}" "${LOCAL_SYNC_CACHE_MOCK}"
    sudo cp -a "${SHM_DIR}" "${LOCAL_SYNC_CACHE_MOCK}"

    ls -lah "${DATADIR}"
    ls -lah "${DATADIR}/blockchain"

    ls -lah "${LOCAL_SYNC_CACHE_MOCK}"
    ls -lah "${LOCAL_SYNC_CACHE_MOCK}/datadir" || true
    ls -lah "${LOCAL_SYNC_CACHE_MOCK}/datadir/blockchain" || true
    ls -lah "${LOCAL_SYNC_CACHE_MOCK}/shm_dir" || true

    # Push mock sync data to NFS cache for sharing across builders
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    if [[ -x "$CACHE_MANAGER" ]]; then
      echo "Pushing mock sync data to NFS cache: ${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY_MOCK}"
      "$CACHE_MANAGER" put "${SYNC_CACHE_TYPE}" "${SYNC_CACHE_KEY_MOCK}" "${LOCAL_SYNC_CACHE_MOCK}" || echo "Warning: Failed to push to NFS cache"
    else
      echo "Warning: cache-manager.sh not found, skipping NFS cache push"
    fi

    # Manually remove the copy of the replay data to preserve disk space on the replay server
    sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

    echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always
  tags:
  - data-cache-storage
  - fast

# HAF instance with NFS fallback for sync data via copy_datadir.sh
# copy_datadir.sh has resolve_data_source() which extracts from NFS if local cache not found
.haf-instance-with-nfs-fallback: &haf-instance-with-nfs-fallback
  name: ${HAF_IMAGE_NAME}
  alias: haf-instance
  variables:
    PGCTLTIMEOUT: 600
    PG_ACCESS: |
      "host    all              haf_admin        0.0.0.0/0    trust"
      "host    all              hived            0.0.0.0/0    trust"
      "host    all              btracker_user       0.0.0.0/0    trust"
      "host    all              btracker_owner      0.0.0.0/0    trust"
      "host    all              all              0.0.0.0/0    scram-sha-256"
    DATA_SOURCE: "/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY}"
    SHARED_BLOCK_LOG_DIR: "/blockchain/block_log_5m"
  command: ["--execute-maintenance-script=${HAF_SOURCE_DIR}/scripts/maintenance-scripts/sleep_infinity.sh"]

# HAF instance with NFS fallback for mock sync data via copy_datadir.sh
# copy_datadir.sh has resolve_data_source() which extracts from NFS if local cache not found
.haf-instance-with-nfs-fallback-mock: &haf-instance-with-nfs-fallback-mock
  name: ${HAF_IMAGE_NAME}
  alias: haf-instance
  variables:
    PGCTLTIMEOUT: 600
    PG_ACCESS: |
      "host    all              haf_admin        0.0.0.0/0    trust"
      "host    all              hived            0.0.0.0/0    trust"
      "host    all              btracker_user       0.0.0.0/0    trust"
      "host    all              btracker_owner      0.0.0.0/0    trust"
      "host    all              all              0.0.0.0/0    scram-sha-256"
    DATA_SOURCE: "/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY_MOCK}"
    SHARED_BLOCK_LOG_DIR: "/blockchain/block_log_5m"
  command: ["--execute-maintenance-script=${HAF_SOURCE_DIR}/scripts/maintenance-scripts/sleep_infinity.sh"]

.postgrest-service: &postgrest-service
  name: registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
  alias: postgrest-server
  variables:
    PGRST_ADMIN_SERVER_PORT: 3001
    PGRST_SERVER_PORT: 3000
    # Pointing to the PostgreSQL service running in haf-instance
    PGRST_DB_URI: postgresql://haf_admin@haf-instance:5432/haf_block_log
    PGRST_DB_SCHEMA: btracker_endpoints
    PGRST_DB_ANON_ROLE: btracker_user
    PGRST_DB_POOL: 20
    PGRST_DB_POOL_ACQUISITION_TIMEOUT: 10
    PGRST_DB_EXTRA_SEARCH_PATH: btracker_app
    HEALTHCHECK_TCP_PORT: 3000

# Before script to extract NFS cache for service containers
# Service containers wait for this extraction to complete
.extract-nfs-cache-before-script: &extract-nfs-cache-before-script
  - |
    LOCAL_CACHE="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY}"
    NFS_TAR="${DATA_CACHE_NFS_PREFIX}/${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY}.tar"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: NFS_TAR=${NFS_TAR}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"
    echo "DEBUG: NFS tar exists: $([[ -f "$NFS_TAR" ]] && echo yes || echo no)"
    if [[ ! -d "${LOCAL_CACHE}/datadir" ]] && [[ -f "$NFS_TAR" ]]; then
      echo "Extracting NFS cache for service container..."
      mkdir -p "${LOCAL_CACHE}"
      tar xf "$NFS_TAR" -C "${LOCAL_CACHE}"
      echo "Extraction complete: ${LOCAL_CACHE}"
    else
      echo "Skipping extraction (local cache exists or NFS tar not found)"
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"

# Combined before script for pytest jobs: NFS extraction + pytest setup
.extract-nfs-cache-pytest-before-script: &extract-nfs-cache-pytest-before-script
  - |
    LOCAL_CACHE="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY}"
    NFS_TAR="${DATA_CACHE_NFS_PREFIX}/${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY}.tar"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: NFS_TAR=${NFS_TAR}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"
    echo "DEBUG: NFS tar exists: $([[ -f "$NFS_TAR" ]] && echo yes || echo no)"
    if [[ ! -d "${LOCAL_CACHE}/datadir" ]] && [[ -f "$NFS_TAR" ]]; then
      echo "Extracting NFS cache for service container..."
      mkdir -p "${LOCAL_CACHE}"
      tar xf "$NFS_TAR" -C "${LOCAL_CACHE}"
      echo "Extraction complete: ${LOCAL_CACHE}"
    else
      echo "Skipping extraction (local cache exists or NFS tar not found)"
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"
  - python3 -m venv venv/
  - . venv/bin/activate
  - echo "Entering ${POETRY_INSTALL_ROOT_DIR}"
  - cd "${POETRY_INSTALL_ROOT_DIR}"
  - poetry install

# Before script for mock data extraction
.extract-nfs-cache-mock-before-script: &extract-nfs-cache-mock-before-script
  - |
    LOCAL_CACHE="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY_MOCK}"
    NFS_TAR="${DATA_CACHE_NFS_PREFIX}/${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY_MOCK}.tar"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: NFS_TAR=${NFS_TAR}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"
    echo "DEBUG: NFS tar exists: $([[ -f "$NFS_TAR" ]] && echo yes || echo no)"
    if [[ ! -d "${LOCAL_CACHE}/datadir" ]] && [[ -f "$NFS_TAR" ]]; then
      echo "Extracting NFS cache for service container (mock)..."
      mkdir -p "${LOCAL_CACHE}"
      tar xf "$NFS_TAR" -C "${LOCAL_CACHE}"
      echo "Extraction complete: ${LOCAL_CACHE}"
    else
      echo "Skipping extraction (local cache exists or NFS tar not found)"
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"

# Combined before script for pytest jobs with mock data: NFS extraction + pytest setup
.extract-nfs-cache-mock-pytest-before-script: &extract-nfs-cache-mock-pytest-before-script
  - |
    LOCAL_CACHE="/cache/${SYNC_CACHE_TYPE}_${SYNC_CACHE_KEY_MOCK}"
    NFS_TAR="${DATA_CACHE_NFS_PREFIX}/${SYNC_CACHE_TYPE}/${SYNC_CACHE_KEY_MOCK}.tar"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: NFS_TAR=${NFS_TAR}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"
    echo "DEBUG: NFS tar exists: $([[ -f "$NFS_TAR" ]] && echo yes || echo no)"
    if [[ ! -d "${LOCAL_CACHE}/datadir" ]] && [[ -f "$NFS_TAR" ]]; then
      echo "Extracting NFS cache for service container (mock)..."
      mkdir -p "${LOCAL_CACHE}"
      tar xf "$NFS_TAR" -C "${LOCAL_CACHE}"
      echo "Extraction complete: ${LOCAL_CACHE}"
    else
      echo "Skipping extraction (local cache exists or NFS tar not found)"
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"
  - python3 -m venv venv/
  - . venv/bin/activate
  - echo "Entering ${POETRY_INSTALL_ROOT_DIR}"
  - cd "${POETRY_INSTALL_ROOT_DIR}"
  - poetry install

regression-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-11
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    cd tests/account_balances
    ./accounts_dump_test.sh --host=haf-instance

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
    - tests/account_balances/account_dump_test.log
    when: always
  tags:
  - data-cache-storage
  - fast

setup-scripts-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-11
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    cd tests/functional
    ./test_scripts.sh --host=haf-instance

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  tags:
  - data-cache-storage
  - fast

performance-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-11
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  - *postgrest-service
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    timeout -k 1m 10m ./balance-tracker.sh run-tests --backend-host=postgrest-server --postgres-host=haf-instance
    tar -czvf tests/performance/results.tar.gz $(pwd)/tests/performance/*result.*
    cat jmeter.log | python3 docker/ci/parse-jmeter-output.py
    m2u --input $(pwd)/tests/performance/result.xml --output $(pwd)/tests/performance/junit-result.xml

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    - tests/performance/result_report/
    - tests/performance/results.tar.gz
    - jmeter.log
    reports:
      junit: tests/performance/junit-result.xml
  tags:
  - data-cache-storage
  - fast

pattern-test:
  extends: .pytest_based_template
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  - *postgrest-service
  before_script: *extract-nfs-cache-pytest-before-script
  variables:
    JUNIT_REPORT: $CI_PROJECT_DIR/tests/tavern/report.xml
    PYTEST_BASED_IMAGE_NAME: $BUILDER_IMAGE_PATH
    POETRY_INSTALL_ROOT_DIR: $CI_PROJECT_DIR/haf/hive/tests/python/hive-local-tools
    BTRACKER_ADDRESS: postgrest-server
    BTRACKER_PORT: 3000
    TAVERN_DIR: $CI_PROJECT_DIR/tests/tavern
  script:
  - |
    cd $CI_PROJECT_DIR/tests/tavern
    pytest -n $PYTEST_NUMBER_OF_PROCESSES --junitxml report.xml .
  artifacts:
    paths:
    - "**/*.out.json"
  tags:
  - data-cache-storage
  - fast

pattern-test-with-mock-data:
  extends: .pytest_based_template
  stage: test
  needs:
  - job: sync_with_mock_data
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback-mock
  - *postgrest-service
  before_script: *extract-nfs-cache-mock-pytest-before-script
  variables:
    JUNIT_REPORT: $CI_PROJECT_DIR/tests/tavern_mocks/report.xml
    PYTEST_BASED_IMAGE_NAME: $BUILDER_IMAGE_PATH
    POETRY_INSTALL_ROOT_DIR: $CI_PROJECT_DIR/haf/hive/tests/python/hive-local-tools
    BTRACKER_ADDRESS: postgrest-server
    BTRACKER_PORT: 3000
    TAVERN_DIR: $CI_PROJECT_DIR/tests/tavern_mocks
  script:
  - |
    cd $CI_PROJECT_DIR/tests/tavern_mocks
    pytest -n $PYTEST_NUMBER_OF_PROCESSES --junitxml report.xml .
  artifacts:
    paths:
    - "**/*.out.json"
  tags:
  - data-cache-storage
  - fast

python_api_client_test:
  extends: .project_develop_configuration_template
  stage: test
  needs:
  - job: generate_python_api_client
    artifacts: true
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  script:
  - pytest "${PYPROJECT_DIR}/tests"
  tags:
  - public-runner-docker

build_and_publish_image:
  stage: publish
  extends: .publish_docker_image_template
  before_script:
  - !reference [.publish_docker_image_template, before_script]
  script:
  - |
    scripts/ci-helpers/build_and_publish_instance.sh
    if [[ -n "$CI_COMMIT_TAG" ]]; then
      docker tag "$CI_REGISTRY_IMAGE/postgrest-rewriter:$CI_COMMIT_TAG" "registry-upload.hive.blog/balance_tracker/postgrest-rewriter:$CI_COMMIT_TAG"
      docker push "registry-upload.hive.blog/balance_tracker/postgrest-rewriter:$CI_COMMIT_TAG"
    fi
  tags:
  - public-runner-docker
  - build-mainnet

deploy_python_api_packages_to_gitlab:
  stage: publish
  needs:
  - job: build_python_api_client_wheel
    artifacts: true
  - job: generate_python_api_client
    artifacts: true
  - job: python_api_client_test
  extends: .deploy_wheel_to_gitlab_template
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  when: on_success
  tags:
  - public-runner-docker

deploy-wax-spec-dev-package:
  extends: .npm_deploy_package_template
  stage: publish
  variables:
    SOURCE_DIR: "${PACKAGE_SOURCE_DIR}"
    PACKAGE_TGZ_PATH: "${BUILT_PACKAGE_PATH}"
    NPM_PACKAGE_SCOPE: "@hiveio"
  needs:
  - job: generate-wax-spec
    artifacts: true
  tags:
  - public-runner-docker

deploy-wax-spec-production-public-npm:
  extends: .registry_npmjs_org_deploy_package_template
  stage: publish
  variables:
    NPM_PUBLISH_TOKEN: "$INTERNAL_HIDDEN_PUBLISH_TOKEN"
    NPM_PACKAGE_NAME: "wax-api-balance-tracker"
    SOURCE_DIR: "${PACKAGE_SOURCE_DIR}"
    PACKAGE_TGZ_PATH: "${BUILT_PACKAGE_PATH}"
  needs:
  - job: generate-wax-spec
    artifacts: true
  tags:
  - public-runner-docker

cleanup_haf_cache_manual:
  extends: .cleanup_cache_manual_template
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "${DATA_CACHE_HAF_PREFIX}_*"
  tags:
  - data-cache-storage
  - fast
