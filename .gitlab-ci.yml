stages:
- lint
- build
- sync
- test
- cleanup
- publish

variables:
  # Git configuration
  GIT_STRATEGY: clone
  GIT_SUBMODULE_STRATEGY: recursive
  GIT_DEPTH: 1
  GIT_SUBMODULE_DEPTH: 1
  GIT_SUBMODULE_UPDATE_FLAGS: --jobs 4
  # HAF configuration
  DATA_CACHE_HAF_PREFIX: "/cache/replay_data_haf"
  # NFS cache configuration for sync data sharing across builders
  DATA_CACHE_NFS_PREFIX: "/nfs/ci-cache"
  # Cache keys include both HAF and balance_tracker commits for proper invalidation
  BTRACKER_CACHE_KEY: "${HAF_COMMIT}_${CI_COMMIT_SHORT_SHA}"
  # Cache types for different replay states
  BTRACKER_SYNC_CACHE_TYPE: "btracker_sync"   # HAF + balance_tracker synced
  BTRACKER_MOCK_CACHE_TYPE: "btracker_mock"   # HAF + balance_tracker + mock data
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m
  FF_NETWORK_PER_BUILD: 1
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu24.04-10
  BUILDER_IMAGE_TAG: "$TEST_HAF_IMAGE_TAG"
  BUILDER_IMAGE_PATH: "registry.gitlab.syncad.com/hive/haf/ci-base-image${BUILDER_IMAGE_TAG}"
  # HAF submodule commit - must match the 'ref:' in the include section below
  # This is needed for service containers which can't access dotenv artifacts
  HAF_COMMIT: "48407d1a5c08bef22d4cab7262e96e4202b8d99a"
  # Enable CI-specific PostgreSQL config with reduced memory for HAF service containers
  HAF_CI_MODE: "1"

include:
- template: Workflows/Branch-Pipelines.gitlab-ci.yml
- project: hive/haf
  ref: 48407d1a5c08bef22d4cab7262e96e4202b8d99a   # feature/generic-cache-handling
  file: /scripts/ci-helpers/prepare_data_image_job.yml
  # Do not include common-ci-configuration here, it is already referenced by scripts/ci-helpers/prepare_data_image_job.yml included from Haf/Hive repos

.lint_job:
  stage: lint
  variables:
    GIT_SUBMODULE_STRATEGY: none
  artifacts:
    name: lint-results
    when: always
  tags:
  - public-runner-docker

lint_bash_scripts:
  extends: .lint_job
  image: koalaman/shellcheck-alpine:latest
  before_script:
  - apk add xmlstarlet
  script:
  - find . -name .git -type d -prune -o -type f -name \*.sh -exec shellcheck -f checkstyle
    {} + | tee shellcheck-checkstyle-result.xml
  after_script:
  - xmlstarlet tr misc/checkstyle2junit.xslt shellcheck-checkstyle-result.xml > shellcheck-junit-result.xml
  artifacts:
    paths:
    - shellcheck-checkstyle-result.xml
    - shellcheck-junit-result.xml
    reports:
      junit: shellcheck-junit-result.xml

lint_sql_scripts:
  extends: .lint_job
  image:
    name: sqlfluff/sqlfluff:2.1.4
    entrypoint: [""]
  script:
  - sqlfluff lint --format yaml --write-output sql-lint.yaml
  artifacts:
    paths:
    - sql-lint.yaml

validate_haf_commit:
  stage: build
  image: alpine:latest
  script:
  - |
    set -e
    apk add --no-cache git
    # Validate that HAF_COMMIT variable matches both the submodule and include ref
    # This prevents cache misses due to mismatched commits
    SUBMODULE_COMMIT=$(cat .git/modules/haf/HEAD 2>/dev/null || git -C haf rev-parse HEAD)
    INCLUDE_REF=$(grep -A2 "project:.*hive/haf" .gitlab-ci.yml | grep "ref:" | head -1 | sed 's/.*ref: *\([a-f0-9]*\).*/\1/' || true)

    echo "HAF_COMMIT variable: $HAF_COMMIT"
    echo "HAF submodule HEAD:  $SUBMODULE_COMMIT"
    echo "Include ref:         $INCLUDE_REF"

    ERRORS=0
    if [ "$HAF_COMMIT" != "$SUBMODULE_COMMIT" ]; then
      echo "ERROR: HAF_COMMIT variable does not match submodule commit!"
      echo "       Update HAF_COMMIT in .gitlab-ci.yml to: $SUBMODULE_COMMIT"
      ERRORS=1
    fi
    if [ "$HAF_COMMIT" != "$INCLUDE_REF" ]; then
      echo "ERROR: HAF_COMMIT variable does not match include ref!"
      echo "       Both should be: $HAF_COMMIT"
      ERRORS=1
    fi
    if [ $ERRORS -eq 1 ]; then
      echo ""
      echo "To fix: ensure HAF_COMMIT, include ref, and submodule all use the same commit"
      exit 1
    fi
    echo "All HAF commit references are consistent"
  tags:
  - public-runner-docker

prepare_haf_image:
  stage: build
  extends: .prepare_haf_image
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$HAF_DEPLOY_USERNAME"
    REGISTRY_PASS: "$HAF_DEPLOY_TOKEN"
  before_script:
  - git config --global --add safe.directory $CI_PROJECT_DIR/haf
  tags:
  - public-runner-docker
  - build-mainnet

extract-swagger-json:
  extends: .filter_out_swagger_json
  stage: build
  variables:
    INPUT_SQL_SWAGGER_FILE: "${CI_PROJECT_DIR}/endpoints/endpoint_schema.sql"
  tags:
  - public-runner-docker

generate_python_api_client:
  extends: .project_develop_configuration_template
  stage: build
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  needs:
  - job: extract-swagger-json
    artifacts: true
  script:
  - ${PYPROJECT_DIR}/generate_balance_tracker_api_client.sh
  artifacts:
    paths:
    - "${PYPROJECT_DIR}/balance_api"
    - "${PYPROJECT_DIR}/balance_api/balance_api_client"
  tags:
  - public-runner-docker

build_python_api_client_wheel:
  extends: .build_wheel_template
  stage: build
  needs:
  - job: generate_python_api_client
    artifacts: true
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  tags:
  - public-runner-docker

generate-wax-spec:
  extends: .generate_swagger_package
  stage: build
  variables:
    # Use artifact path directly - dotenv BUILT_JSON_SWAGGER_FILE contains absolute
    # path from previous job's runner which doesn't exist on this runner
    INPUT_JSON_SWAGGER_FILE: "${CI_PROJECT_DIR}/build/swagger-doc.json"
    API_TYPE: "rest"
    NAMESPACE: "balance_tracker"
    NPM_PACKAGE_SCOPE: "@hiveio"
    NPM_PACKAGE_NAME: "wax-api-balance-tracker"
  needs:
  - job: extract-swagger-json
    artifacts: true
  tags:
  - public-runner-docker

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
  - job: prepare_haf_image
    artifacts: true
  stage: build
  timeout: 80m
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/docker/config_5M.ini"
  tags:
  - data-cache-storage
  - fast

.docker-build-template:
  extends: .docker_image_builder_job_template
  stage: build
  variables:
    BASE_REPO_NAME: ""
    BASE_TAG: ""
    NAME: ""
    TARGET: "$NAME"
    PROGRESS_DISPLAY: "plain"
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  script:
  - |
    echo -e "\e[0Ksection_end:$(date +%s):tag\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):build[collapsed=true]\r\e[0KBaking $NAME${BASE_REPO_NAME:+/$BASE_REPO_NAME} image..."
    function image-exists() {
      local image=$1
      docker manifest inspect "$1" > /dev/null
      return $?
    }
    if image-exists "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:$BASE_TAG"; then
      echo "Image $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG} already exists. Skipping..."
      if [[ -n "$CI_COMMIT_TAG" && "$TARGET" == "full-ci" ]]; then
        echo "Tagging pre-existing image with Git tag..."
        docker pull "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG}"
        docker tag "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG}" "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
        docker push "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
      fi
    else
      echo "Baking $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${BASE_TAG} image..."
      git config --global --add safe.directory $(pwd)
      scripts/ci-helpers/build_docker_image.sh "$CI_PROJECT_DIR"
    fi
    echo -e "\e[0Ksection_end:$(date +%s):build\r\e[0K"
  tags:
  - public-runner-docker
  - build-mainnet

docker-ci-runner-build:
  extends: .docker-build-template
  variables:
    BASE_REPO_NAME: ""
    BASE_TAG: "docker-24.0.1-16"
    NAME: "ci-runner"
    TARGET: "ci-runner-ci"

docker-setup-docker-image-build:
  extends: .docker-build-template
  variables:
    GIT_SUBMODULE_STRATEGY: none
    GIT_DEPTH: 1
    BASE_REPO_NAME: ""
    BASE_TAG: "$CI_COMMIT_SHORT_SHA"
    NAME: ""
    TARGET: "full-ci"

sync:
  extends: .docker_image_builder_job_template
  stage: sync
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - prepare_haf_image
  - prepare_haf_data
  - docker-setup-docker-image-build
  - docker-ci-runner-build
  variables:
    DATA_SOURCE: ${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMPOSE_OPTIONS_STRING: --env-file ci.env --file docker-compose.yml --file overrides/ci.yml
      --ansi never
  timeout: 1 hours
  before_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
    git config --global --add safe.directory "$CI_PROJECT_DIR"
    git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
    echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  - |
    # Check for existing btracker sync cache first (HAF + balance_tracker already synced)
    # If found, we can skip the sync entirely since the cache key includes both commits
    LOCAL_BTRACKER_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    LOCAL_HAF_CACHE="${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    CACHE_HIT="false"

    if [[ -d "${LOCAL_BTRACKER_CACHE}/datadir" ]]; then
      echo "Local btracker sync cache found at ${LOCAL_BTRACKER_CACHE} - skipping sync"
      CACHE_HIT="true"
      export DATA_SOURCE="${LOCAL_BTRACKER_CACHE}"
    elif [[ -x "$CACHE_MANAGER" ]]; then
      echo "Checking NFS for btracker sync cache: ${BTRACKER_SYNC_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_BTRACKER_CACHE}" 2>/dev/null; then
        echo "Fetched btracker sync cache from NFS - skipping sync"
        CACHE_HIT="true"
        export DATA_SOURCE="${LOCAL_BTRACKER_CACHE}"
      else
        echo "Btracker sync cache not found, will use HAF-only cache and run sync"
      fi
    fi

    # Fall back to HAF-only cache if no btracker cache
    if [[ "$CACHE_HIT" != "true" ]]; then
      if [[ -d "${LOCAL_HAF_CACHE}/datadir" ]]; then
        echo "Local HAF cache found at ${LOCAL_HAF_CACHE}"
      else
        echo "Local HAF cache not found, checking NFS..."
        if [[ -x "$CACHE_MANAGER" ]]; then
          if "$CACHE_MANAGER" get haf "${HAF_COMMIT}" "${LOCAL_HAF_CACHE}"; then
            echo "Fetched HAF replay data from NFS cache"
          else
            echo "ERROR: Failed to fetch HAF replay data from NFS cache"
            exit 1
          fi
        else
          echo "ERROR: cache-manager.sh not found and local cache missing"
          exit 1
        fi
      fi
    fi

    # Export for use in script section
    echo "$CACHE_HIT" > /tmp/cache_hit
  script:
  - |
    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

    # Block log is already in HAF cache from prepare_haf_data, copy_datadir.sh handles it
    "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

    # Using --skip-hived: pre-replayed HAF data from NFS cache contains both
    # PostgreSQL database (pgdata) and shared_memory.bin. HAF just runs PostgreSQL
    # without hived, using the existing replayed data. No need to delete pgdata.

    # Docker Compose bind mounts docker/blockchain over datadir/blockchain.
    # Copy blockchain files there so HAF can see them.
    # Use cp -aL to dereference symlinks (copy actual files, not symlink references)
    # This is needed because datadir/blockchain may contain symlinks to /cache/blockchain/block_log_5m/
    # which isn't accessible inside the Docker Compose containers (DinD environment)
    echo "Copying blockchain files to docker/blockchain..."
    rm -rf "${CI_PROJECT_DIR}/docker/blockchain"/*
    cp -aL "${DATADIR}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Contents of docker/blockchain after copy:"
    ls -la "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Total size:"
    du -sh "${CI_PROJECT_DIR}/docker/blockchain/"

    # Remove blockchain dir from datadir so Docker volume mount doesn't conflict with bind mount
    # Docker Compose will use ./blockchain bind mount exclusively for /home/hived/datadir/blockchain
    sudo rm -rf "${DATADIR}/blockchain"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/start-ci-test-environment.sh"

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"

    if [[ "$CACHE_HIT" == "true" ]]; then
      echo "Cache hit - balance_tracker already synced, skipping wait"
    else
      echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for Balance Tracker to sync..."
      "${CI_PROJECT_DIR}/scripts/ci-helpers/wait-for-bt-startup.sh"
      echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
    fi
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    pushd docker
    IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING

    # Force PostgreSQL checkpoint before shutdown to ensure all btracker data is written to disk
    # Without this, data may be lost when cache-manager excludes WAL files
    echo "Forcing PostgreSQL checkpoint..."
    docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" || true

    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-setup > backend-setup.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > backend-postgrest.log

    # Use longer timeout (60s) for graceful PostgreSQL shutdown
    docker compose "${COMPOSE_OPTIONS[@]}" down --volumes --timeout 60
    popd

    tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log

    # Only save to cache if this was not a cache hit (avoid re-saving what we loaded)
    if [[ "$CACHE_HIT" != "true" ]]; then
      # Save sync data to local cache with commit-based key
      LOCAL_BTRACKER_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
      mkdir -p "${LOCAL_BTRACKER_CACHE}"
      sudo cp -a "${DATADIR}" "${LOCAL_BTRACKER_CACHE}"
      sudo cp -a "${SHM_DIR}" "${LOCAL_BTRACKER_CACHE}"

      # Remove empty blockchain from local cache to trigger symlink creation on test runners
      if [[ -d "${LOCAL_BTRACKER_CACHE}/datadir/blockchain" ]] && [[ -z "$(ls -A "${LOCAL_BTRACKER_CACHE}/datadir/blockchain")" ]]; then
        echo "Removing empty blockchain directory from local cache"
        rmdir "${LOCAL_BTRACKER_CACHE}/datadir/blockchain"
      fi

      ls -lah "${LOCAL_BTRACKER_CACHE}"
      ls -lah "${LOCAL_BTRACKER_CACHE}/datadir" || true
      ls -lah "${LOCAL_BTRACKER_CACHE}/shm_dir" || true

      # Push sync data to NFS cache for sharing across builders
      CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
      if [[ -x "$CACHE_MANAGER" ]]; then
        echo "Pushing btracker sync data to NFS: ${BTRACKER_SYNC_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
        CACHE_HANDLING=haf "$CACHE_MANAGER" put "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_BTRACKER_CACHE}" || echo "Warning: Failed to push to NFS cache"
      else
        echo "Warning: cache-manager.sh not found, skipping NFS cache push"
      fi
    else
      echo "Cache hit - skipping cache save (data already in cache)"
    fi

    # Manually remove the copy of the replay data to preserve disk space on the replay server
    sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

    echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always
  tags:
  - data-cache-storage
  - fast

sync_with_mock_data:
  extends: .docker_image_builder_job_template
  stage: sync
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  - job: docker-ci-runner-build
    artifacts: true
  variables:
    DATA_SOURCE: ${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMPOSE_OPTIONS_STRING: --env-file ci.env --file docker-compose-mocks.yml --file
      overrides/ci.yml --ansi never
  timeout: 1 hours
  before_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
    echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
    git config --global --add safe.directory "$CI_PROJECT_DIR"
    git config --global --add safe.directory "$CI_PROJECT_DIR/haf"
    echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  - |
    # Check for existing mock cache first (HAF + btracker + mock data already present)
    LOCAL_MOCK_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    LOCAL_BTRACKER_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    CACHE_HIT="false"

    # First check for mock cache (already has mock data)
    if [[ -d "${LOCAL_MOCK_CACHE}/datadir" ]]; then
      echo "Local mock cache found at ${LOCAL_MOCK_CACHE} - skipping mock fill"
      CACHE_HIT="true"
      export DATA_SOURCE="${LOCAL_MOCK_CACHE}"
    elif [[ -x "$CACHE_MANAGER" ]]; then
      echo "Checking NFS for mock cache: ${BTRACKER_MOCK_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_MOCK_CACHE}" 2>/dev/null; then
        echo "Fetched mock cache from NFS - skipping mock fill"
        CACHE_HIT="true"
        export DATA_SOURCE="${LOCAL_MOCK_CACHE}"
      else
        echo "Mock cache not found, will use btracker sync cache and fill mock data"
      fi
    fi

    # Fall back to btracker sync cache if no mock cache
    if [[ "$CACHE_HIT" != "true" ]]; then
      # Check if local btracker cache exists and is usable
      LOCAL_CACHE_VALID=false
      if [[ -d "${LOCAL_BTRACKER_CACHE}/datadir" ]]; then
        # Check for broken symlinks in pg_tblspc
        TBLSPC="${LOCAL_BTRACKER_CACHE}/datadir/haf_db_store/pgdata/pg_tblspc"
        if [[ -d "$TBLSPC" ]]; then
          BROKEN_LINKS=0
          shopt -s nullglob
          for link in "$TBLSPC"/*; do
            if [[ -L "$link" ]] && [[ ! -e "$link" ]]; then
              BROKEN_LINKS=$((BROKEN_LINKS + 1))
            fi
          done
          shopt -u nullglob
          if [[ "$BROKEN_LINKS" -gt 0 ]]; then
            echo "Local cache has $BROKEN_LINKS broken symlinks, will re-extract"
            sudo rm -rf "${LOCAL_BTRACKER_CACHE}" || rm -rf "${LOCAL_BTRACKER_CACHE}" || true
          else
            LOCAL_CACHE_VALID=true
          fi
        else
          LOCAL_CACHE_VALID=true
        fi
      fi

      if [[ "$LOCAL_CACHE_VALID" == "true" ]]; then
        echo "Local btracker sync cache found at ${LOCAL_BTRACKER_CACHE}"
      else
        echo "Local btracker cache not found or invalid, fetching from NFS..."
        if [[ -x "$CACHE_MANAGER" ]]; then
          if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_BTRACKER_CACHE}"; then
            echo "Fetched btracker sync data from NFS via cache-manager"
          else
            echo "ERROR: Failed to fetch btracker sync data from NFS"
            exit 1
          fi
        else
          echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
          exit 1
        fi
      fi
    fi

    # Export for use in script section
    echo "$CACHE_HIT" > /tmp/cache_hit
  script:
  - |
    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

    # Block log is already in cache, copy_datadir.sh handles it
    "${CI_PROJECT_DIR}/haf/scripts/copy_datadir.sh"

    # Restore PostgreSQL pgdata permissions after copy from cache
    # The cache has relaxed permissions (a+rX) for copying, but PostgreSQL requires mode 700
    PGDATA_PATH="${DATADIR}/haf_db_store/pgdata"
    if [[ -d "$PGDATA_PATH" ]]; then
      echo "Restoring pgdata permissions to mode 700"
      sudo chmod 700 "$PGDATA_PATH"
      sudo chown -R 105:105 "${DATADIR}/haf_db_store"
      ls -la "${DATADIR}/haf_db_store/"
    else
      echo "WARNING: pgdata directory not found at $PGDATA_PATH"
      ls -la "${DATADIR}/" || true
      ls -la "${DATADIR}/haf_db_store/" || true
    fi

    # Docker Compose bind mounts docker/blockchain over datadir/blockchain.
    # Copy blockchain files there so HAF can see them.
    echo "Copying blockchain files to docker/blockchain..."
    rm -rf "${CI_PROJECT_DIR}/docker/blockchain"/*
    cp -aL "${DATADIR}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Contents of docker/blockchain after copy:"
    ls -la "${CI_PROJECT_DIR}/docker/blockchain/"
    echo "Total size:"
    du -sh "${CI_PROJECT_DIR}/docker/blockchain/"

    # Remove blockchain dir from datadir so Docker volume mount doesn't conflict with bind mount
    sudo rm -rf "${DATADIR}/blockchain"

    "${CI_PROJECT_DIR}/scripts/ci-helpers/start-ci-test-environment.sh"

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"

    if [[ "$CACHE_HIT" == "true" ]]; then
      echo "Cache hit - mock data already present, skipping wait"
    else
      echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for mock data fill..."
      "${CI_PROJECT_DIR}/scripts/ci-helpers/wait-for-bt-startup.sh"
      echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
    fi
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

    CACHE_HIT=$(cat /tmp/cache_hit 2>/dev/null || echo "false")

    pushd docker
    IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING

    # Force PostgreSQL checkpoint before shutdown
    echo "Forcing PostgreSQL checkpoint..."
    docker compose "${COMPOSE_OPTIONS[@]}" exec -T haf psql -U haf_admin -d haf_block_log -c "CHECKPOINT;" || true

    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs fill-db-with-mock-data > fill-db-with-mock-data.log
    docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log

    # Use longer timeout (60s) for graceful PostgreSQL shutdown
    docker compose "${COMPOSE_OPTIONS[@]}" down --volumes --timeout 60
    popd

    tar -czvf docker/container-logs.tar.gz $(pwd)/docker/*.log

    # Only save to cache if this was not a cache hit
    if [[ "$CACHE_HIT" != "true" ]]; then
      # Save mock data to local cache with commit-based key
      LOCAL_MOCK_CACHE="${DATA_CACHE_HAF_PREFIX}_${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
      mkdir -p "${LOCAL_MOCK_CACHE}"
      sudo cp -a "${DATADIR}" "${LOCAL_MOCK_CACHE}"
      sudo cp -a "${SHM_DIR}" "${LOCAL_MOCK_CACHE}"

      # Remove empty blockchain from local cache
      if [[ -d "${LOCAL_MOCK_CACHE}/datadir/blockchain" ]] && [[ -z "$(ls -A "${LOCAL_MOCK_CACHE}/datadir/blockchain")" ]]; then
        echo "Removing empty blockchain directory from local cache"
        rmdir "${LOCAL_MOCK_CACHE}/datadir/blockchain"
      fi

      ls -lah "${LOCAL_MOCK_CACHE}"
      ls -lah "${LOCAL_MOCK_CACHE}/datadir" || true
      ls -lah "${LOCAL_MOCK_CACHE}/shm_dir" || true

      # Push mock data to NFS cache for sharing across builders
      CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
      if [[ -x "$CACHE_MANAGER" ]]; then
        echo "Pushing mock data to NFS: ${BTRACKER_MOCK_CACHE_TYPE}/${BTRACKER_CACHE_KEY}"
        CACHE_HANDLING=haf "$CACHE_MANAGER" put "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_MOCK_CACHE}" || echo "Warning: Failed to push to NFS cache"
      else
        echo "Warning: cache-manager.sh not found, skipping NFS cache push"
      fi
    else
      echo "Cache hit - skipping cache save (data already in cache)"
    fi

    # Manually remove the copy of the replay data to preserve disk space
    sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

    echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    expire_in: 1 week
    when: always
  tags:
  - data-cache-storage
  - fast

# HAF instance with NFS fallback for sync data
# Services start before before_script, so we wait for job to extract data if needed
.haf-instance-with-nfs-fallback: &haf-instance-with-nfs-fallback
  name: ${HAF_IMAGE_NAME}
  alias: haf-instance
  variables:
    PGCTLTIMEOUT: 600
    PG_ACCESS: |
      "host    all              haf_admin        0.0.0.0/0    trust"
      "host    all              hived            0.0.0.0/0    trust"
      "host    all              btracker_user       0.0.0.0/0    trust"
      "host    all              btracker_owner      0.0.0.0/0    trust"
      "host    all              all              0.0.0.0/0    scram-sha-256"
    DATA_SOURCE: "/cache/${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    DATA_SOURCE_NFS_PREFIX: "${DATA_CACHE_NFS_PREFIX}"
    DATA_SOURCE_NFS_TYPE: "${BTRACKER_SYNC_CACHE_TYPE}"
    DATA_SOURCE_NFS_KEY: "${BTRACKER_CACHE_KEY}"
  entrypoint:
    - '/bin/bash'
    - '-c'
    - |
      set -xeuo pipefail
      echo "Service container starting..."
      DATA_PATH="${DATA_SOURCE}"
      MARKER_FILE="${DATA_PATH}/.ready"
      NFS_PREFIX="${DATA_SOURCE_NFS_PREFIX:-/nfs/ci-cache}"
      NFS_TYPE="${DATA_SOURCE_NFS_TYPE:-haf_sync}"
      NFS_KEY="${DATA_SOURCE_NFS_KEY}"
      NFS_TAR="${NFS_PREFIX}/${NFS_TYPE}/${NFS_KEY}.tar"

      echo "DEBUG: DATA_PATH=${DATA_PATH}"
      echo "DEBUG: MARKER_FILE=${MARKER_FILE}"
      echo "DEBUG: NFS_TAR=${NFS_TAR}"

      # Remove stale marker file to ensure we wait for fresh extraction
      # (previous runs may have left marker files behind)
      rm -f "${MARKER_FILE}" 2>/dev/null || true

      # Strategy: Wait for job's extraction, then try NFS directly, then fall back to local cache
      # This handles timing issues where service starts before job's before_script runs
      # Note: Job needs ~90s for git clone + cache extraction, so we wait 120s

      # Step 1: Wait up to 120s for job to extract (marker file)
      WAITED=0
      while [[ ! -f "$MARKER_FILE" ]] && [[ $WAITED -lt 120 ]]; do
        echo "Waiting for job extraction... ($WAITED/120s)"
        sleep 5
        WAITED=$((WAITED + 5))
      done

      if [[ -f "$MARKER_FILE" ]]; then
        echo "Marker file found - using job-extracted data"
      elif [[ -f "$NFS_TAR" ]]; then
        # Step 2: Job didn't extract yet, try NFS directly
        echo "Job extraction not ready, extracting from NFS tar directly..."
        mkdir -p "${DATA_PATH}"
        tar xf "$NFS_TAR" -C "${DATA_PATH}" --overwrite || {
          echo "WARNING: NFS tar extraction failed, trying local cache"
        }
        touch "${MARKER_FILE}" 2>/dev/null || true
        echo "NFS extraction complete"
      elif [[ -d "${DATA_PATH}/datadir" ]]; then
        # Step 3: Fall back to existing local cache
        echo "WARNING: Using existing local cache (NFS not available)"
        echo "Data may be stale if sync ran on different builder"
      else
        echo "ERROR: No data available"
        echo "  - Marker file: ${MARKER_FILE} (not found)"
        echo "  - NFS tar: ${NFS_TAR} (not found)"
        echo "  - Local cache: ${DATA_PATH}/datadir (not found)"
        exit 1
      fi

      echo "Contents of ${DATA_PATH}:"
      ls -la "${DATA_PATH}/" 2>/dev/null | head -10

      # Verify datadir exists
      if [[ ! -d "${DATA_PATH}/datadir" ]]; then
        echo "ERROR: datadir not found at ${DATA_PATH}/datadir"
        exit 1
      fi

      export DATA_SOURCE="${DATA_PATH}"

      # Restore pgdata permissions (PostgreSQL requires 700 or 750)
      PGDATA="${DATA_SOURCE}/datadir/haf_db_store/pgdata"
      if [[ -d "$PGDATA" ]]; then
        echo "Restoring pgdata permissions to mode 700"
        chmod 700 "$PGDATA" 2>/dev/null || sudo chmod 700 "$PGDATA" || true
      fi

      # Run original entrypoint
      exec /home/haf_admin/docker_entrypoint.sh "$@"
    - '/bin/bash'
  command: ["--execute-maintenance-script=${HAF_SOURCE_DIR}/scripts/maintenance-scripts/sleep_infinity.sh"]

# HAF instance with NFS fallback for mock sync data
.haf-instance-with-nfs-fallback-mock: &haf-instance-with-nfs-fallback-mock
  name: ${HAF_IMAGE_NAME}
  alias: haf-instance
  variables:
    PGCTLTIMEOUT: 600
    PG_ACCESS: |
      "host    all              haf_admin        0.0.0.0/0    trust"
      "host    all              hived            0.0.0.0/0    trust"
      "host    all              btracker_user       0.0.0.0/0    trust"
      "host    all              btracker_owner      0.0.0.0/0    trust"
      "host    all              all              0.0.0.0/0    scram-sha-256"
    DATA_SOURCE: "/cache/${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    DATA_SOURCE_NFS_PREFIX: "${DATA_CACHE_NFS_PREFIX}"
    DATA_SOURCE_NFS_TYPE: "${BTRACKER_MOCK_CACHE_TYPE}"
    DATA_SOURCE_NFS_KEY: "${BTRACKER_CACHE_KEY}"
  entrypoint:
    - '/bin/bash'
    - '-c'
    - |
      set -xeuo pipefail
      echo "Service container starting..."
      DATA_PATH="${DATA_SOURCE}"
      MARKER_FILE="${DATA_PATH}/.ready"
      NFS_PREFIX="${DATA_SOURCE_NFS_PREFIX:-/nfs/ci-cache}"
      NFS_TYPE="${DATA_SOURCE_NFS_TYPE:-haf_sync}"
      NFS_KEY="${DATA_SOURCE_NFS_KEY}"
      NFS_TAR="${NFS_PREFIX}/${NFS_TYPE}/${NFS_KEY}.tar"

      echo "DEBUG: DATA_PATH=${DATA_PATH}"
      echo "DEBUG: MARKER_FILE=${MARKER_FILE}"
      echo "DEBUG: NFS_TAR=${NFS_TAR}"

      # Remove stale marker file to ensure we wait for fresh extraction
      # (previous runs may have left marker files behind)
      rm -f "${MARKER_FILE}" 2>/dev/null || true

      # Strategy: Wait for job's extraction, then try NFS directly, then fall back to local cache
      # This handles timing issues where service starts before job's before_script runs
      # Note: Job needs ~90s for git clone + cache extraction, so we wait 120s

      # Step 1: Wait up to 120s for job to extract (marker file)
      WAITED=0
      while [[ ! -f "$MARKER_FILE" ]] && [[ $WAITED -lt 120 ]]; do
        echo "Waiting for job extraction... ($WAITED/120s)"
        sleep 5
        WAITED=$((WAITED + 5))
      done

      if [[ -f "$MARKER_FILE" ]]; then
        echo "Marker file found - using job-extracted data"
      elif [[ -f "$NFS_TAR" ]]; then
        # Step 2: Job didn't extract yet, try NFS directly
        echo "Job extraction not ready, extracting from NFS tar directly..."
        mkdir -p "${DATA_PATH}"
        tar xf "$NFS_TAR" -C "${DATA_PATH}" --overwrite || {
          echo "WARNING: NFS tar extraction failed, trying local cache"
        }
        touch "${MARKER_FILE}" 2>/dev/null || true
        echo "NFS extraction complete"
      elif [[ -d "${DATA_PATH}/datadir" ]]; then
        # Step 3: Fall back to existing local cache
        echo "WARNING: Using existing local cache (NFS not available)"
        echo "Data may be stale if sync ran on different builder"
      else
        echo "ERROR: No data available"
        echo "  - Marker file: ${MARKER_FILE} (not found)"
        echo "  - NFS tar: ${NFS_TAR} (not found)"
        echo "  - Local cache: ${DATA_PATH}/datadir (not found)"
        exit 1
      fi

      echo "Contents of ${DATA_PATH}:"
      ls -la "${DATA_PATH}/" 2>/dev/null | head -10

      # Verify datadir exists
      if [[ ! -d "${DATA_PATH}/datadir" ]]; then
        echo "ERROR: datadir not found at ${DATA_PATH}/datadir"
        exit 1
      fi

      export DATA_SOURCE="${DATA_PATH}"

      # Restore pgdata permissions (PostgreSQL requires 700 or 750)
      PGDATA="${DATA_SOURCE}/datadir/haf_db_store/pgdata"
      if [[ -d "$PGDATA" ]]; then
        echo "Restoring pgdata permissions to mode 700"
        chmod 700 "$PGDATA" 2>/dev/null || sudo chmod 700 "$PGDATA" || true
      fi

      # Run original entrypoint
      exec /home/haf_admin/docker_entrypoint.sh "$@"
    - '/bin/bash'
  command: ["--execute-maintenance-script=${HAF_SOURCE_DIR}/scripts/maintenance-scripts/sleep_infinity.sh"]

.postgrest-service: &postgrest-service
  name: registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
  alias: postgrest-server
  variables:
    PGRST_ADMIN_SERVER_PORT: 3001
    PGRST_SERVER_PORT: 3000
    # Pointing to the PostgreSQL service running in haf-instance
    PGRST_DB_URI: postgresql://haf_admin@haf-instance:5432/haf_block_log
    PGRST_DB_SCHEMA: btracker_endpoints
    PGRST_DB_ANON_ROLE: btracker_user
    PGRST_DB_POOL: 20
    PGRST_DB_POOL_ACQUISITION_TIMEOUT: 10
    PGRST_DB_EXTRA_SEARCH_PATH: btracker_app
    HEALTHCHECK_TCP_PORT: 3000

# Before script to extract NFS cache for service containers
# Service containers wait for the .ready marker file before starting PostgreSQL
.extract-nfs-cache-before-script: &extract-nfs-cache-before-script
  - |
    LOCAL_CACHE="/cache/${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    MARKER_FILE="${LOCAL_CACHE}/.ready"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_SYNC_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"
    echo "DEBUG: MARKER_FILE=${MARKER_FILE}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"

    # Check if marker file has the current pipeline ID (means fresh extraction was done)
    MARKER_PIPELINE=""
    if [[ -f "$MARKER_FILE" ]]; then
      MARKER_PIPELINE=$(cat "$MARKER_FILE" 2>/dev/null || echo "")
    fi
    echo "DEBUG: Marker pipeline: ${MARKER_PIPELINE:-none} (current: ${CI_PIPELINE_ID})"

    # Check if PostgreSQL is already running (service container started with data)
    PG_RUNNING=false
    if pg_isready -h haf-instance -p 5432 -q 2>/dev/null; then
      PG_RUNNING=true
      echo "DEBUG: PostgreSQL is running - will skip extraction"
    fi

    # Extract fresh data only if:
    # 1. Marker doesn't have current pipeline ID AND
    # 2. PostgreSQL is NOT running (otherwise files are in use)
    if [[ "$MARKER_PIPELINE" != "${CI_PIPELINE_ID}" ]] && [[ "$PG_RUNNING" != "true" ]]; then
      echo "Fetching cache via cache-manager..."
      # Remove existing cache to avoid permission errors on overwrite
      sudo rm -rf "${LOCAL_CACHE}" 2>/dev/null || rm -rf "${LOCAL_CACHE}" 2>/dev/null || true
      if [[ -x "$CACHE_MANAGER" ]]; then
        if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_CACHE}"; then
          echo "Cache extraction complete: ${LOCAL_CACHE}"
          echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}"
          echo "Created marker file: ${MARKER_FILE} (pipeline ${CI_PIPELINE_ID})"
        else
          echo "ERROR: cache-manager failed to get cache"
          exit 1
        fi
      else
        echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
        exit 1
      fi
    elif [[ "$PG_RUNNING" == "true" ]]; then
      echo "PostgreSQL already running - using existing data (files in use)"
      echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}" 2>/dev/null || true
    elif [[ -d "${LOCAL_CACHE}/datadir" ]]; then
      echo "Using existing local cache (marker already set)"
    else
      echo "ERROR: No cache available"
      exit 1
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"

# Combined before script for pytest jobs: NFS extraction + pytest setup
.extract-nfs-cache-pytest-before-script: &extract-nfs-cache-pytest-before-script
  - |
    LOCAL_CACHE="/cache/${BTRACKER_SYNC_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    MARKER_FILE="${LOCAL_CACHE}/.ready"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_SYNC_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"
    echo "DEBUG: MARKER_FILE=${MARKER_FILE}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"

    # Check if marker file has the current pipeline ID (means fresh extraction was done)
    MARKER_PIPELINE=""
    if [[ -f "$MARKER_FILE" ]]; then
      MARKER_PIPELINE=$(cat "$MARKER_FILE" 2>/dev/null || echo "")
    fi
    echo "DEBUG: Marker pipeline: ${MARKER_PIPELINE:-none} (current: ${CI_PIPELINE_ID})"

    # Check if PostgreSQL is already running (service container started with data)
    PG_RUNNING=false
    if pg_isready -h haf-instance -p 5432 -q 2>/dev/null; then
      PG_RUNNING=true
      echo "DEBUG: PostgreSQL is running - will skip extraction"
    fi

    # Extract fresh data only if:
    # 1. Marker doesn't have current pipeline ID AND
    # 2. PostgreSQL is NOT running (otherwise files are in use)
    if [[ "$MARKER_PIPELINE" != "${CI_PIPELINE_ID}" ]] && [[ "$PG_RUNNING" != "true" ]]; then
      echo "Fetching cache via cache-manager..."
      # Remove existing cache to avoid permission errors on overwrite
      sudo rm -rf "${LOCAL_CACHE}" 2>/dev/null || rm -rf "${LOCAL_CACHE}" 2>/dev/null || true
      if [[ -x "$CACHE_MANAGER" ]]; then
        if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_CACHE}"; then
          echo "Cache extraction complete: ${LOCAL_CACHE}"
          echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}"
          echo "Created marker file: ${MARKER_FILE} (pipeline ${CI_PIPELINE_ID})"
        else
          echo "ERROR: cache-manager failed to get cache"
          exit 1
        fi
      else
        echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
        exit 1
      fi
    elif [[ "$PG_RUNNING" == "true" ]]; then
      echo "PostgreSQL already running - using existing data (files in use)"
      echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}" 2>/dev/null || true
    elif [[ -d "${LOCAL_CACHE}/datadir" ]]; then
      echo "Using existing local cache (marker already set)"
    else
      echo "ERROR: No cache available"
      exit 1
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"
  - python3 -m venv venv/
  - . venv/bin/activate
  - echo "Entering ${POETRY_INSTALL_ROOT_DIR}"
  - cd "${POETRY_INSTALL_ROOT_DIR}"
  - poetry install

# Before script for mock data extraction
.extract-nfs-cache-mock-before-script: &extract-nfs-cache-mock-before-script
  - |
    LOCAL_CACHE="/cache/${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    MARKER_FILE="${LOCAL_CACHE}/.ready"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_MOCK_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"
    echo "DEBUG: MARKER_FILE=${MARKER_FILE}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"

    # Check if marker file has the current pipeline ID (means fresh extraction was done)
    MARKER_PIPELINE=""
    if [[ -f "$MARKER_FILE" ]]; then
      MARKER_PIPELINE=$(cat "$MARKER_FILE" 2>/dev/null || echo "")
    fi
    echo "DEBUG: Marker pipeline: ${MARKER_PIPELINE:-none} (current: ${CI_PIPELINE_ID})"

    # Check if PostgreSQL is already running (service container started with data)
    PG_RUNNING=false
    if pg_isready -h haf-instance -p 5432 -q 2>/dev/null; then
      PG_RUNNING=true
      echo "DEBUG: PostgreSQL is running - will skip extraction"
    fi

    # Extract fresh data only if:
    # 1. Marker doesn't have current pipeline ID AND
    # 2. PostgreSQL is NOT running (otherwise files are in use)
    if [[ "$MARKER_PIPELINE" != "${CI_PIPELINE_ID}" ]] && [[ "$PG_RUNNING" != "true" ]]; then
      echo "Fetching cache via cache-manager (mock)..."
      # Remove existing cache to avoid permission errors on overwrite
      sudo rm -rf "${LOCAL_CACHE}" 2>/dev/null || rm -rf "${LOCAL_CACHE}" 2>/dev/null || true
      if [[ -x "$CACHE_MANAGER" ]]; then
        if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_CACHE}"; then
          echo "Cache extraction complete: ${LOCAL_CACHE}"
          echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}"
          echo "Created marker file: ${MARKER_FILE} (pipeline ${CI_PIPELINE_ID})"
        else
          echo "ERROR: cache-manager failed to get cache"
          exit 1
        fi
      else
        echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
        exit 1
      fi
    elif [[ "$PG_RUNNING" == "true" ]]; then
      echo "PostgreSQL already running - using existing data (files in use)"
      echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}" 2>/dev/null || true
    elif [[ -d "${LOCAL_CACHE}/datadir" ]]; then
      echo "Using existing local cache (marker already set)"
    else
      echo "ERROR: No cache available"
      exit 1
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"

# Combined before script for pytest jobs with mock data: NFS extraction + pytest setup
.extract-nfs-cache-mock-pytest-before-script: &extract-nfs-cache-mock-pytest-before-script
  - |
    LOCAL_CACHE="/cache/${BTRACKER_MOCK_CACHE_TYPE}_${BTRACKER_CACHE_KEY}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"
    MARKER_FILE="${LOCAL_CACHE}/.ready"
    echo "DEBUG: LOCAL_CACHE=${LOCAL_CACHE}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_MOCK_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"
    echo "DEBUG: MARKER_FILE=${MARKER_FILE}"
    echo "DEBUG: Local cache exists: $([[ -d "${LOCAL_CACHE}/datadir" ]] && echo yes || echo no)"

    # Check if marker file has the current pipeline ID (means fresh extraction was done)
    MARKER_PIPELINE=""
    if [[ -f "$MARKER_FILE" ]]; then
      MARKER_PIPELINE=$(cat "$MARKER_FILE" 2>/dev/null || echo "")
    fi
    echo "DEBUG: Marker pipeline: ${MARKER_PIPELINE:-none} (current: ${CI_PIPELINE_ID})"

    # Check if PostgreSQL is already running (service container started with data)
    PG_RUNNING=false
    if pg_isready -h haf-instance -p 5432 -q 2>/dev/null; then
      PG_RUNNING=true
      echo "DEBUG: PostgreSQL is running - will skip extraction"
    fi

    # Extract fresh data only if:
    # 1. Marker doesn't have current pipeline ID AND
    # 2. PostgreSQL is NOT running (otherwise files are in use)
    if [[ "$MARKER_PIPELINE" != "${CI_PIPELINE_ID}" ]] && [[ "$PG_RUNNING" != "true" ]]; then
      echo "Fetching cache via cache-manager (mock pytest)..."
      # Remove existing cache to avoid permission errors on overwrite
      sudo rm -rf "${LOCAL_CACHE}" 2>/dev/null || rm -rf "${LOCAL_CACHE}" 2>/dev/null || true
      if [[ -x "$CACHE_MANAGER" ]]; then
        if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${LOCAL_CACHE}"; then
          echo "Cache extraction complete: ${LOCAL_CACHE}"
          echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}"
          echo "Created marker file: ${MARKER_FILE} (pipeline ${CI_PIPELINE_ID})"
        else
          echo "ERROR: cache-manager failed to get cache"
          exit 1
        fi
      else
        echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
        exit 1
      fi
    elif [[ "$PG_RUNNING" == "true" ]]; then
      echo "PostgreSQL already running - using existing data (files in use)"
      echo "${CI_PIPELINE_ID}" > "${MARKER_FILE}" 2>/dev/null || true
    elif [[ -d "${LOCAL_CACHE}/datadir" ]]; then
      echo "Using existing local cache (marker already set)"
    else
      echo "ERROR: No cache available"
      exit 1
    fi
  - |
    # Wait for haf-instance PostgreSQL to be ready
    echo "Waiting for haf-instance PostgreSQL service..."
    WAIT_TIMEOUT=300
    WAIT_INTERVAL=2
    WAITED=0
    while ! pg_isready -h haf-instance -p 5432 -q 2>/dev/null; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for haf-instance after ${WAIT_TIMEOUT}s"
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgreSQL... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgreSQL is ready!"
  - python3 -m venv venv/
  - . venv/bin/activate
  - echo "Entering ${POETRY_INSTALL_ROOT_DIR}"
  - cd "${POETRY_INSTALL_ROOT_DIR}"
  - poetry install

# regression-test uses Docker-in-Docker to avoid service container race conditions
# This ensures NFS data is extracted BEFORE containers start
regression-test:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  variables:
    HAF_DATA_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    HAF_SHM_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    COMPOSE_OPTIONS_STRING: --file docker-compose-test.yml --ansi never
  timeout: 30m
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting NFS cache..."

    JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"

    echo "DEBUG: JOB_DIR=${JOB_DIR}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_SYNC_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"

    # Use cache-manager to get data (handles local cache check + NFS tar extraction)
    mkdir -p "${JOB_DIR}"
    if [[ -x "$CACHE_MANAGER" ]]; then
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${JOB_DIR}"; then
        echo "Cache extraction complete"
      else
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi
    else
      echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
      exit 1
    fi

    # Handle blockchain symlinks - dereference and copy to docker dir
    if [[ -L "${HAF_DATA_DIRECTORY}/blockchain" ]] || [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]]; then
      echo "Copying blockchain data to docker directory..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true
      rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
    fi

    ls -la "${HAF_DATA_DIRECTORY}/"
    ls -la "${HAF_SHM_DIRECTORY}/" || true

    echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting test environment..."

    cd "${CI_PROJECT_DIR}/docker"

    # Create ci.env for docker-compose
    cat <<-EOF | tee ci.env
    HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
    HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
    HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
    HIVED_UID=$(id -u)
    POSTGREST_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
    EOF

    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services to be ready..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Wait for HAF PostgreSQL to be healthy
    TIMEOUT=300
    ELAPSED=0
    until pg_isready -h docker -p 5432 -U haf_admin -d haf_block_log 2>/dev/null; do
      sleep 1
      ELAPSED=$((ELAPSED + 1))
      if [ $ELAPSED -ge $TIMEOUT ]; then
        echo "Timeout waiting for PostgreSQL"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs
        exit 1
      fi
    done
    echo "PostgreSQL is ready!"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    cd "${CI_PROJECT_DIR}/tests/regression"
    # In DinD, PostgreSQL is available at 'docker' host
    ./run_test.sh --host=docker

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up test environment..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Capture container logs before cleanup
    docker compose "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true

    # Stop and remove containers
    docker compose "${COMPOSE_OPTIONS[@]}" down -v --remove-orphans || true

    # Archive logs
    tar -czvf container-logs.tar.gz *.log 2>/dev/null || true

    echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"
  artifacts:
    paths:
    - tests/regression/regression_test.log
    - docker/container-logs.tar.gz
    when: always
  tags:
  - data-cache-storage

setup-scripts-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    cd tests/functional
    ./test_scripts.sh --host=haf-instance

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  tags:
  - data-cache-storage
  - fast

performance-test:
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  stage: test
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  services:
  - *haf-instance-with-nfs-fallback
  - *postgrest-service
  before_script: *extract-nfs-cache-before-script
  script:
  - |
    # Wait for PostgREST to be ready (healthcheck on port 3000)
    echo "Waiting for PostgREST service..."
    WAIT_TIMEOUT=120
    WAIT_INTERVAL=2
    WAITED=0
    while ! curl -sf http://postgrest-server:3000/ > /dev/null 2>&1; do
      if [[ $WAITED -ge $WAIT_TIMEOUT ]]; then
        echo "ERROR: Timed out waiting for PostgREST after ${WAIT_TIMEOUT}s"
        echo "Checking if service is reachable..."
        curl -v http://postgrest-server:3000/ || true
        exit 1
      fi
      sleep $WAIT_INTERVAL
      WAITED=$((WAITED + WAIT_INTERVAL))
      echo "  Waiting for PostgREST... ($WAITED/${WAIT_TIMEOUT}s)"
    done
    echo "PostgREST is ready!"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):tests\r\e[0KRunning tests..."

    timeout -k 1m 10m ./scripts/ci-helpers/run_performance_tests.sh --backend-host=postgrest-server --postgres-host=haf-instance
    tar -czvf tests/performance/results.tar.gz $(pwd)/tests/performance/*result.*
    cat jmeter.log | python3 docker/ci/parse-jmeter-output.py
    m2u --input $(pwd)/tests/performance/result.xml --output $(pwd)/tests/performance/junit-result.xml

    echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
    - docker/container-logs.tar.gz
    - tests/performance/result_report/
    - tests/performance/results.tar.gz
    - jmeter.log
    reports:
      junit: tests/performance/junit-result.xml
  tags:
  - data-cache-storage
  - fast

# pattern-test uses Docker-in-Docker to avoid service container race conditions
# This ensures NFS data is extracted BEFORE containers start
pattern-test:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  variables:
    HAF_DATA_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    HAF_SHM_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    COMPOSE_OPTIONS_STRING: --file docker-compose-test.yml --ansi never
    JUNIT_REPORT: $CI_PROJECT_DIR/tests/tavern/report.xml
    BTRACKER_ADDRESS: localhost
    BTRACKER_PORT: 3000
    TAVERN_DIR: $CI_PROJECT_DIR/tests/tavern
    PYTEST_NUMBER_OF_PROCESSES: 16
  timeout: 30m
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting NFS cache..."

    JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"

    echo "DEBUG: JOB_DIR=${JOB_DIR}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_SYNC_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"

    # Use cache-manager to get data (handles local cache check + NFS tar extraction)
    mkdir -p "${JOB_DIR}"
    if [[ -x "$CACHE_MANAGER" ]]; then
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_SYNC_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${JOB_DIR}"; then
        echo "Cache extraction complete"
      else
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi
    else
      echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
      exit 1
    fi

    # Handle blockchain symlinks - dereference and copy to docker dir
    if [[ -L "${HAF_DATA_DIRECTORY}/blockchain" ]] || [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]]; then
      echo "Copying blockchain data to docker directory..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true
      rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
    fi

    ls -la "${HAF_DATA_DIRECTORY}/"
    ls -la "${HAF_SHM_DIRECTORY}/" || true

    echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting test environment..."

    cd "${CI_PROJECT_DIR}/docker"

    # Create ci.env for docker-compose
    cat <<-EOF | tee ci.env
    HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
    HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
    HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
    HIVED_UID=$(id -u)
    POSTGREST_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
    EOF

    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services to be ready..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Wait for HAF PostgreSQL to be healthy
    WAITED=0
    MAX_WAIT=300
    until docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" exec -T haf pg_isready -U haf_admin -d haf_block_log 2>/dev/null; do
      echo "Waiting for PostgreSQL... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgreSQL did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf | tail -50
        exit 1
      fi
    done
    echo "PostgreSQL is ready!"

    # Wait for PostgREST to be ready
    # In DinD, exposed ports are available at 'docker' host, not localhost
    WAITED=0
    until curl -s http://docker:3000/ > /dev/null 2>&1; do
      echo "Waiting for PostgREST at docker:3000... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgREST did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest | tail -50
        exit 1
      fi
    done
    echo "PostgREST is ready at docker:3000!"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):venv[collapsed=true]\r\e[0KSetting up Python environment..."

    # Install test dependencies directly (avoid full hive-local-tools which has incompatible hiveio-wax wheel)
    python3 -m venv venv/
    . venv/bin/activate

    echo "Installing test dependencies..."
    pip install tavern==2.2.0 pytest-xdist==3.2.0 pyyaml requests deepdiff prettytable
    # Install tests_api for validate_response module (required by tavern tests)
    pip install -e "${CI_PROJECT_DIR}/haf/hive/tests/python/hive-local-tools/tests_api"

    echo -e "\e[0Ksection_end:$(date +%s):venv\r\e[0K"
  script:
  - |
    . venv/bin/activate
    cd $CI_PROJECT_DIR/tests/tavern
    # In DinD, PostgREST is available at 'docker' host
    BTRACKER_ADDRESS=docker pytest -n $PYTEST_NUMBER_OF_PROCESSES --junitxml report.xml .
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up test environment..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Capture logs before stopping
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest > postgrest.log 2>&1 || true

    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" down --volumes || true

    tar -czvf container-logs.tar.gz haf.log postgrest.log 2>/dev/null || true

    # Clean up job-specific data directory
    sudo rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || true

    echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"
  artifacts:
    paths:
    - "**/*.out.json"
    - docker/container-logs.tar.gz
    reports:
      junit: tests/tavern/report.xml
    when: always
  tags:
  - data-cache-storage
  - fast

# pattern-test-with-mock-data uses Docker-in-Docker to avoid service container race conditions
# This ensures NFS data is extracted BEFORE containers start
pattern-test-with-mock-data:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/balance_tracker/ci-runner:docker-24.0.1-16
  needs:
  - job: sync_with_mock_data
    artifacts: true
  - job: docker-setup-docker-image-build
    artifacts: true
  - job: prepare_haf_image
    artifacts: true
  variables:
    HAF_DATA_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    HAF_SHM_DIRECTORY: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    COMPOSE_OPTIONS_STRING: --file docker-compose-test.yml --ansi never
    JUNIT_REPORT: $CI_PROJECT_DIR/tests/tavern_mocks/report.xml
    BTRACKER_ADDRESS: localhost
    BTRACKER_PORT: 3000
    TAVERN_DIR: $CI_PROJECT_DIR/tests/tavern_mocks
    PYTEST_NUMBER_OF_PROCESSES: 16
  timeout: 30m
  before_script:
  - !reference [.docker_image_builder_job_template, before_script]
  - |
    echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
    docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
    echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):extract[collapsed=true]\r\e[0KExtracting NFS cache (mock data)..."

    JOB_DIR="${CI_PROJECT_DIR}/${CI_JOB_ID}"
    CACHE_MANAGER="${CI_PROJECT_DIR}/haf/scripts/ci-helpers/cache-manager.sh"

    echo "DEBUG: JOB_DIR=${JOB_DIR}"
    echo "DEBUG: CACHE_TYPE=${BTRACKER_MOCK_CACHE_TYPE}"
    echo "DEBUG: CACHE_KEY=${BTRACKER_CACHE_KEY}"

    # Use cache-manager to get data (handles local cache check + NFS tar extraction)
    mkdir -p "${JOB_DIR}"
    if [[ -x "$CACHE_MANAGER" ]]; then
      if CACHE_HANDLING=haf "$CACHE_MANAGER" get "${BTRACKER_MOCK_CACHE_TYPE}" "${BTRACKER_CACHE_KEY}" "${JOB_DIR}"; then
        echo "Cache extraction complete"
      else
        echo "ERROR: Failed to get cache via cache-manager"
        exit 1
      fi
    else
      echo "ERROR: cache-manager.sh not found at $CACHE_MANAGER"
      exit 1
    fi

    # Handle blockchain symlinks - dereference and copy to docker dir
    if [[ -L "${HAF_DATA_DIRECTORY}/blockchain" ]] || [[ -d "${HAF_DATA_DIRECTORY}/blockchain" ]]; then
      echo "Copying blockchain data to docker directory..."
      mkdir -p "${CI_PROJECT_DIR}/docker/blockchain"
      cp -aL "${HAF_DATA_DIRECTORY}/blockchain"/* "${CI_PROJECT_DIR}/docker/blockchain/" 2>/dev/null || true
      rm -rf "${HAF_DATA_DIRECTORY}/blockchain"
    fi

    ls -la "${HAF_DATA_DIRECTORY}/"
    ls -la "${HAF_SHM_DIRECTORY}/" || true

    echo -e "\e[0Ksection_end:$(date +%s):extract\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting test environment..."

    cd "${CI_PROJECT_DIR}/docker"

    # Create ci.env for docker-compose
    cat <<-EOF | tee ci.env
    HAF_IMAGE_NAME=${HAF_IMAGE_NAME}
    HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}
    HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}
    HIVED_UID=$(id -u)
    POSTGREST_IMAGE=registry.gitlab.syncad.com/hive/haf_api_node/postgrest:latest
    EOF

    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" config
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" up --detach --quiet-pull

    echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0KWaiting for services to be ready..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Wait for HAF PostgreSQL to be healthy
    WAITED=0
    MAX_WAIT=300
    until docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" exec -T haf pg_isready -U haf_admin -d haf_block_log 2>/dev/null; do
      echo "Waiting for PostgreSQL... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgreSQL did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf | tail -50
        exit 1
      fi
    done
    echo "PostgreSQL is ready!"

    # Wait for PostgREST to be ready
    # In DinD, exposed ports are available at 'docker' host, not localhost
    WAITED=0
    until curl -s http://docker:3000/ > /dev/null 2>&1; do
      echo "Waiting for PostgREST at docker:3000... ($WAITED/$MAX_WAIT seconds)"
      sleep 5
      WAITED=$((WAITED + 5))
      if [[ $WAITED -ge $MAX_WAIT ]]; then
        echo "ERROR: PostgREST did not become ready in time"
        docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest | tail -50
        exit 1
      fi
    done
    echo "PostgREST is ready at docker:3000!"

    echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  - |
    echo -e "\e[0Ksection_start:$(date +%s):venv[collapsed=true]\r\e[0KSetting up Python environment..."

    # Install test dependencies directly (avoid full hive-local-tools which has incompatible hiveio-wax wheel)
    python3 -m venv venv/
    . venv/bin/activate

    echo "Installing test dependencies..."
    pip install tavern==2.2.0 pytest-xdist==3.2.0 pyyaml requests deepdiff prettytable
    # Install tests_api for validate_response module (required by tavern tests)
    pip install -e "${CI_PROJECT_DIR}/haf/hive/tests/python/hive-local-tools/tests_api"

    echo -e "\e[0Ksection_end:$(date +%s):venv\r\e[0K"
  script:
  - |
    . venv/bin/activate
    cd $CI_PROJECT_DIR/tests/tavern_mocks
    # In DinD, PostgREST is available at 'docker' host
    BTRACKER_ADDRESS=docker pytest -n $PYTEST_NUMBER_OF_PROCESSES --junitxml report.xml .
  after_script:
  - |
    echo -e "\e[0Ksection_start:$(date +%s):cleanup[collapsed=true]\r\e[0KCleaning up test environment..."

    cd "${CI_PROJECT_DIR}/docker"
    IFS=" " read -ra COMPOSE_OPTIONS <<< "$COMPOSE_OPTIONS_STRING"

    # Capture logs before stopping
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs haf > haf.log 2>&1 || true
    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" logs postgrest > postgrest.log 2>&1 || true

    docker compose --env-file ci.env "${COMPOSE_OPTIONS[@]}" down --volumes || true

    tar -czvf container-logs.tar.gz haf.log postgrest.log 2>/dev/null || true

    # Clean up job-specific data directory
    sudo rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || rm -rf "${CI_PROJECT_DIR}/${CI_JOB_ID}" 2>/dev/null || true

    echo -e "\e[0Ksection_end:$(date +%s):cleanup\r\e[0K"
  artifacts:
    paths:
    - "**/*.out.json"
    - docker/container-logs.tar.gz
    reports:
      junit: tests/tavern_mocks/report.xml
    when: always
  tags:
  - data-cache-storage
  - fast

python_api_client_test:
  extends: .project_develop_configuration_template
  stage: test
  needs:
  - job: generate_python_api_client
    artifacts: true
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  script:
  - pytest "${PYPROJECT_DIR}/tests"
  tags:
  - public-runner-docker

build_and_publish_image:
  stage: publish
  extends: .publish_docker_image_template
  before_script:
  - !reference [.publish_docker_image_template, before_script]
  script:
  - |
    scripts/ci-helpers/build_and_publish_instance.sh
    if [[ -n "$CI_COMMIT_TAG" ]]; then
      docker tag "$CI_REGISTRY_IMAGE/postgrest-rewriter:$CI_COMMIT_TAG" "registry-upload.hive.blog/balance_tracker/postgrest-rewriter:$CI_COMMIT_TAG"
      docker push "registry-upload.hive.blog/balance_tracker/postgrest-rewriter:$CI_COMMIT_TAG"
    fi
  tags:
  - public-runner-docker
  - build-mainnet

deploy_python_api_packages_to_gitlab:
  stage: publish
  needs:
  - job: build_python_api_client_wheel
    artifacts: true
  - job: generate_python_api_client
    artifacts: true
  - job: python_api_client_test
  extends: .deploy_wheel_to_gitlab_template
  variables:
    PYPROJECT_DIR: "${CI_PROJECT_DIR}/scripts/python_api_package"
  when: on_success
  tags:
  - public-runner-docker

deploy-wax-spec-dev-package:
  extends: .npm_deploy_package_template
  stage: publish
  variables:
    # Use artifact paths directly - dotenv vars contain absolute paths from previous
    # job's runner which don't exist on this runner
    SOURCE_DIR: "${CI_PROJECT_DIR}/build/generated"
    NPM_PACKAGE_SCOPE: "@hiveio"
  before_script:
    - git config --global --add safe.directory '*'
    - . "${NVM_DIR}/nvm.sh"
    - nvm use "${NODEJS_VERSION}"
    - cd "${SOURCE_DIR}"
    - pnpm --recursive install --frozen-lockfile
    # Find the tgz file - filename includes version number so we can't hardcode it
    - export PACKAGE_TGZ_PATH=$(find "${CI_PROJECT_DIR}/build" -name "*.tgz" | head -1)
    - echo "Found package at ${PACKAGE_TGZ_PATH}"
    - echo -e "\e[0Ksection_start:$(date +%s):package_tgz_unpack[collapsed=true]\r\e[0KAttempting to unpack ${PACKAGE_TGZ_PATH}"
    - cd "${CI_PROJECT_DIR}"
    - tar -xf "${PACKAGE_TGZ_PATH}" -C "${SOURCE_DIR}" --strip-components=1
    - echo -e "\e[0Ksection_end:$(date +%s):package_tgz_unpack\r\e[0K"
  needs:
  - job: generate-wax-spec
    artifacts: true
  tags:
  - public-runner-docker

deploy-wax-spec-production-public-npm:
  extends: .registry_npmjs_org_deploy_package_template
  stage: publish
  variables:
    NPM_PUBLISH_TOKEN: "$INTERNAL_HIDDEN_PUBLISH_TOKEN"
    NPM_PACKAGE_NAME: "wax-api-balance-tracker"
    # Use artifact paths directly - dotenv vars contain absolute paths from previous
    # job's runner which don't exist on this runner
    SOURCE_DIR: "${CI_PROJECT_DIR}/build/generated"
  before_script:
    - git config --global --add safe.directory '*'
    - . "${NVM_DIR}/nvm.sh"
    - nvm use "${NODEJS_VERSION}"
    - cd "${SOURCE_DIR}"
    - pnpm --recursive install --frozen-lockfile
    # Find the tgz file - filename includes version number so we can't hardcode it
    - export PACKAGE_TGZ_PATH=$(find "${CI_PROJECT_DIR}/build" -name "*.tgz" | head -1)
    - echo "Found package at ${PACKAGE_TGZ_PATH}"
    - echo -e "\e[0Ksection_start:$(date +%s):package_tgz_unpack[collapsed=true]\r\e[0KAttempting to unpack ${PACKAGE_TGZ_PATH}"
    - cd "${CI_PROJECT_DIR}"
    - tar -xf "${PACKAGE_TGZ_PATH}" -C "${SOURCE_DIR}" --strip-components=1
    - echo -e "\e[0Ksection_end:$(date +%s):package_tgz_unpack\r\e[0K"
  needs:
  - job: generate-wax-spec
    artifacts: true
  tags:
  - public-runner-docker

cleanup_haf_cache_manual:
  extends: .cleanup_cache_manual_template
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "${DATA_CACHE_HAF_PREFIX}_*"
  tags:
  - data-cache-storage
  - fast
